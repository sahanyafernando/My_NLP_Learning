# üåç NLP Project: Multilingual Public Response Analysis
## üìå Project Overview

This project focuses on analyzing multilingual public responses to government policies 
and events using Natural Language Processing (NLP). Governments often introduce reforms 
or respond to crises, which trigger large-scale discussions across platforms such as
Twitter, Facebook, and Reddit. These conversations‚Äîespecially when expressed in multiple
languages‚Äîare critical for understanding public sentiment, support trends, concerns, 
and potential misinformation across time and regions.

This project simulates such real-world scenarios using a multilingual social media 
dataset and applies both classical and advanced NLP techniques to extract insights.

## üéØ Main Objective
To develop an end-to-end multilingual NLP pipeline capable of:
  - Cleaning and preprocessing noisy social media text
  - Extracting linguistic and semantic features
  - Performing sentiment analysis and topic modeling
  - Tracking temporal and event-based sentiment shifts
  - Summarizing large volumes of public discourse

## üìÇ Project Data (What we analyze)
- **Dataset file**: `data/nlp_multilingual_policy_dataset.csv`
- **Example columns**: `post_id`, `user_id`, `platform`, `language`, `timestamp`, `region`, `text`, `topic`, `sentiment_label`, `event_tag`

## üß≠ Notebook Walkthrough (01 ‚Üí 06)
Run the notebooks in order. Notebooks `02`‚Äì`06` reload artifacts generated by `01`.

### 01 ‚Äî Data Loading & Preprocessing (`01_data_loading_and_preprocessing.ipynb`)
**Goal**: Load the multilingual dataset, clean/normalize text, and create reusable feature representations for downstream modeling.

**What I do**
- **Load and inspect** the CSV with `pandas` (head/info/describe).
- **Preprocess text** per row (language-aware):
  - Lowercasing
  - Unicode normalization (`unicodedata`)
  - Regex cleaning to remove punctuation/special characters
  - Tokenization (simple whitespace split)
  - Stopword removal using **NLTK** stopword lists where available (mapped for `en`, `es`, `fr`, `de`, `pt`)
- **Explore frequent terms**: top-N token frequencies per language.
- **Create feature matrices** from the preprocessed strings:
  - One-hot encoding (binary) with `sklearn.feature_extraction.text.CountVectorizer(binary=True)`
  - Bag-of-Words (counts) with `CountVectorizer()`
  - TF-IDF with `TfidfVectorizer()`
  - Bigram co-occurrence matrix with `CountVectorizer(ngram_range=(2,2))`
- **Build a co-occurrence network graph** from bigrams using `networkx` and visualize it with `matplotlib`.

**Key outputs**
- Adds columns like `cleaned_tokens` and `preprocessed_text_string` to the dataframe.
- Saves reusable artifacts to `artifacts/preprocessing_outputs.pkl`:
  - `df`
  - `one_hot_vectorizer`, `one_hot_matrix`
  - `bow_vectorizer`, `bow_matrix`
  - `tfidf_vectorizer`, `tfidf_matrix`
  - `cooccurrence_vectorizer`, `cooccurrence_matrix`

### 02 ‚Äî Embeddings & Topic Modeling (`02_embeddings_and_topic_modeling.ipynb`)
**Goal**: Generate multiple embedding types (dense + lexical) and extract latent themes via topic modeling.

**What I do**
- **Load preprocessing artifacts** produced by notebook `01`.
- **Dense embeddings with Gensim**:
  - Train **Word2Vec** on `cleaned_tokens` and inspect vectors + most-similar terms.
  - Train **FastText** on `cleaned_tokens` and inspect vectors + most-similar terms.
- **Topic modeling**:
  - Apply **NMF (Non-Negative Matrix Factorization)** on the TF-IDF matrix and interpret topics via top words.
- **Pretrained lexical embeddings (GloVe)**:
  - Download `glove.6B.100d` (zip), extract, load embeddings into memory.
  - Compute cosine similarity to find nearest neighbors for a sample term.
- **Subword/BPE embeddings**:
  - Use `tokenizers.BertWordPieceTokenizer` to build WordPiece/BPE-style tokens from the corpus.
  - Train Word2Vec on BPE tokens and inspect subword similarities.
- **Simple language-model-style embeddings**:
  - Build **unigram probability ‚Äúembeddings‚Äù** from token counts and compare similar-probability words.
- **Hybrid character+word embeddings**:
  - Create character n-gram vectors (`CountVectorizer(analyzer='char', ngram_range=(2,3))`)
  - Concatenate char n-grams with Word2Vec vectors to form hybrid representations.

### 03 ‚Äî Clustering & Sentiment Scoring (`03_clustering_and_sentiment.ipynb`)
**Goal**: Group posts by similarity and compute sentiment polarity scores.

**What I do**
- **Load preprocessing artifacts** produced by notebook `01`.
- **K-Means clustering** on the TF-IDF matrix (`sklearn.cluster.KMeans`) to assign cluster labels.
- **Cluster interpretation** by printing top TF-IDF terms per cluster.
- **Sentiment scoring** using **NLTK VADER** (`nltk.sentiment.vader.SentimentIntensityAnalyzer`) to generate polarity scores (best suited for English; used here as a baseline).

### 04 ‚Äî Supervised Sentiment Classification (`04_sentiment_classification.ipynb`)
**Goal**: Train and evaluate classical ML classifiers to predict `sentiment_label` from TF-IDF features.

**What I do**
- **Load preprocessing artifacts** produced by notebook `01`.
- **Train/test split** with stratification (`sklearn.model_selection.train_test_split`).
- Train and evaluate multiple baselines:
  - **Multinomial Na√Øve Bayes**
  - **Linear SVM (LinearSVC)**
  - **Decision Tree**
- Report **accuracy**, **classification report**, and **confusion matrix**.
- Optional: evaluate performance **per language**.

### 05 ‚Äî NER, Aspect/Topic Sentiment, Temporal Trends (`05_ner_aspect_temporal.ipynb`)
**Goal**: Extract entities, summarize sentiment by topic, and track sentiment over time.

**What I do**
- **Load preprocessing artifacts** produced by notebook `01`.
- **ü§ñ Transformer-based NER** (multilingual):
  - Uses `transformers.pipeline(task="ner", model="Davlan/xlm-roberta-base-ner-hrl")`
  - Model: **XLM-RoBERTa-base** fine-tuned for multilingual Named Entity Recognition (high-resource languages)
  - Extracts entities (PERSON, ORG, LOC, etc.) from posts with aggregation strategy
  - Demonstrates on sample texts from the dataset
- **Simple topic/aspect sentiment view**:
  - For each `topic`, summarize the sentiment label distribution and show example posts.
- **Temporal sentiment analysis**:
  - Convert `timestamp` to datetime and map sentiment labels to numeric scores.
  - Aggregate sentiment over time and visualize trends with `matplotlib`.
  - Flag potential "change points" using a simple mean ¬± std threshold heuristic.

### 06 ‚Äî Summarization (Extractive + Abstractive) (`06_summarization.ipynb`)
**Goal**: Produce concise summaries of public discourse per topic.

**What I do**
- **Load preprocessing artifacts** produced by notebook `01`.
- **Extractive summarization** (English) using TextRank-style summarization from `summa`.
- **ü§ñ Abstractive summarization** (English) using transformers:
  - Uses `transformers.pipeline(task="summarization", model="facebook/bart-large-cnn")`
  - Model: **BART-large-CNN** (fine-tuned on CNN/DailyMail for news-style summarization)
  - Generates abstractive summaries (not just extraction) for individual English posts
  - Configured with `max_length=60, min_length=15` for concise output

### 07 ‚Äî Sentence Transformer-Based Text Classification (`07_sentence_transformer_classification.ipynb`)
**Goal**: Classify sentiment using state-of-the-art sentence transformer embeddings instead of traditional TF-IDF features.

**What I do**
- **Load preprocessing artifacts** produced by notebook `01`.
- **ü§ñ Generate sentence embeddings**:
  - Uses `sentence-transformers` library with model `paraphrase-multilingual-MiniLM-L12-v2`
  - Creates 384-dimensional dense embeddings for each text post
  - Multilingual model supports 50+ languages including all languages in the dataset (en, es, fr, de, hi)
  - Embeds semantic meaning and context, unlike sparse TF-IDF representations
- **Train classification models**:
  - Multiple classifiers: Logistic Regression, SVM (Linear), SVM (RBF), Random Forest
  - Trained on sentence transformer embeddings (dense 384-dim vectors)
  - Evaluated using accuracy, classification report, and confusion matrices
- **Language-wise performance analysis**:
  - Analyzes classification performance across different languages
  - Demonstrates the advantage of multilingual embeddings for cross-language sentiment classification

## üß∞ Libraries Used (Across Notebooks)
Below is the combined set of libraries actually imported / installed in notebooks `01`‚Äì`07`.

### Core Python / utilities
- `pathlib`, `pickle`, `collections`, `os`, `re`, `unicodedata`, `urllib.request`, `zipfile`

### Data handling & math
- `pandas`, `numpy`

### NLP (classical)
- `nltk` (stopwords, VADER sentiment)
- `gensim` (Word2Vec, FastText)
- `tokenizers` (WordPiece/BPE tokenizer)

### ML / feature engineering
- `scikit-learn` (`CountVectorizer`, `TfidfVectorizer`, `NMF`, `KMeans`, `train_test_split`, classifiers: `MultinomialNB`, `LinearSVC`, `DecisionTree`, `LogisticRegression`, `SVC`, `RandomForest`, metrics, cosine similarity)

### Deep NLP / transformers
- `transformers` ‚Äî Hugging Face Transformers library used in:
  - **Notebook 05**: NER pipeline with `Davlan/xlm-roberta-base-ner-hrl` (multilingual XLM-RoBERTa for entity extraction)
  - **Notebook 06**: Summarization pipeline with `facebook/bart-large-cnn` (BART model for abstractive summarization)
- `sentence-transformers` ‚Äî Sentence Transformers library used in:
  - **Notebook 07**: Text classification with `paraphrase-multilingual-MiniLM-L12-v2` (multilingual sentence embeddings)
- `sentencepiece` ‚Äî Tokenization dependency required by transformer models

### Visualization & graphs
- `matplotlib`
- `networkx`

### Summarization
- `summa` (TextRank-style extractive summarization)

### Execution environment (notebook convenience)
- `google.colab` (Drive mounting cells in several notebooks)

## ü§ñ Transformer Models Used
This project uses three pre-trained transformer models:

1. **Notebook 05 ‚Äî NER**: `Davlan/xlm-roberta-base-ner-hrl` (via `transformers` library)
   - **Purpose**: Multilingual Named Entity Recognition
   - **Architecture**: XLM-RoBERTa-base
   - **Supported languages**: Multiple high-resource languages (handles multilingual social media text)
   - **Task**: Extracts entities like persons, organizations, locations from posts

2. **Notebook 06 ‚Äî Summarization**: `facebook/bart-large-cnn` (via `transformers` library)
   - **Purpose**: Abstractive text summarization
   - **Architecture**: BART (Bidirectional Auto-Regressive Transformer) large variant
   - **Training data**: Fine-tuned on CNN/DailyMail dataset
   - **Task**: Generates concise abstractive summaries (not just sentence extraction) for English text

3. **Notebook 07 ‚Äî Text Classification**: `paraphrase-multilingual-MiniLM-L12-v2` (via `sentence-transformers` library)
   - **Purpose**: Multilingual sentence embeddings for text classification
   - **Architecture**: Multilingual MiniLM (Microsoft) based on BERT architecture
   - **Supported languages**: 50+ languages including en, es, fr, de, hi (all languages in our dataset)
   - **Embedding dimension**: 384-dimensional dense vectors
   - **Task**: Creates semantic embeddings for sentiment classification, superior to TF-IDF for multilingual text

**Note**: All models are automatically downloaded from Hugging Face Hub when the notebooks are run.

## ‚úÖ Recommended Run Order
1. `01_data_loading_and_preprocessing.ipynb` (creates `artifacts/preprocessing_outputs.pkl`)
2. `02_embeddings_and_topic_modeling.ipynb`
3. `03_clustering_and_sentiment.ipynb`
4. `04_sentiment_classification.ipynb` (TF-IDF + classical ML baseline)
5. `05_ner_aspect_temporal.ipynb` ‚Üê Uses transformers for NER
6. `06_summarization.ipynb` ‚Üê Uses transformers for summarization
7. `07_sentence_transformer_classification.ipynb` ‚Üê Uses sentence-transformers for classification (compare with notebook 04)