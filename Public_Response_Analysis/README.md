# üåç NLP Project: Multilingual Public Response Analysis
## üìå Project Overview

This project focuses on analyzing multilingual public responses to government policies 
and events using Natural Language Processing (NLP). Governments often introduce reforms 
or respond to crises, which trigger large-scale discussions across platforms such as
Twitter, Facebook, and Reddit. These conversations‚Äîespecially when expressed in multiple
languages‚Äîare critical for understanding public sentiment, support trends, concerns, 
and potential misinformation across time and regions.

This project simulates such real-world scenarios using a multilingual social media 
dataset and applies both classical and advanced NLP techniques to extract insights.

## üéØ Main Objective
To develop an end-to-end multilingual NLP pipeline capable of:
  - Cleaning and preprocessing noisy social media text
  - Extracting linguistic and semantic features
  - Performing sentiment analysis and topic modeling
  - Tracking temporal and event-based sentiment shifts
  - Summarizing large volumes of public discourse

## üß≠ Notebook Walkthrough (01 ‚Üí 06)
Run the notebooks in order. Notebooks `02`‚Äì`06` reload artifacts generated by `01`.

### 01 ‚Äî Data Loading & Preprocessing (`01_data_loading_and_preprocessing.ipynb`)
**Goal**: Load the multilingual dataset, clean/normalize text, and create reusable feature representations for downstream modeling.

**What I do**
- **Load and inspect** the CSV with `pandas` (head/info/describe).
- **Preprocess text** per row (language-aware):
  - Lowercasing
  - Unicode normalization (`unicodedata`)
  - Regex cleaning to remove punctuation/special characters
  - Tokenization (simple whitespace split)
  - Stopword removal using **NLTK** stopword lists where available (mapped for `en`, `es`, `fr`, `de`, `pt`)
- **Explore frequent terms**: top-N token frequencies per language.
- **Create feature matrices** from the preprocessed strings:
  - One-hot encoding (binary) with `sklearn.feature_extraction.text.CountVectorizer(binary=True)`
  - Bag-of-Words (counts) with `CountVectorizer()`
  - TF-IDF with `TfidfVectorizer()`
  - Bigram co-occurrence matrix with `CountVectorizer(ngram_range=(2,2))`
- **Build a co-occurrence network graph** from bigrams using `networkx` and visualize it with `matplotlib`.

**Key outputs**
- Adds columns like `cleaned_tokens` and `preprocessed_text_string` to the dataframe.
- Saves reusable artifacts to `artifacts/preprocessing_outputs.pkl`:
  - `df`
  - `one_hot_vectorizer`, `one_hot_matrix`
  - `bow_vectorizer`, `bow_matrix`
  - `tfidf_vectorizer`, `tfidf_matrix`
  - `cooccurrence_vectorizer`, `cooccurrence_matrix`

### 02 ‚Äî Embeddings & Topic Modeling (`02_embeddings_and_topic_modeling.ipynb`)
**Goal**: Generate multiple embedding types (dense + lexical) and extract latent themes via topic modeling.

**What I do**
- **Load preprocessing artifacts** produced by notebook `01`.
- **Dense embeddings with Gensim**:
  - Train **Word2Vec** on `cleaned_tokens` and inspect vectors + most-similar terms.
  - Train **FastText** on `cleaned_tokens` and inspect vectors + most-similar terms.
- **Topic modeling**:
  - Apply **NMF (Non-Negative Matrix Factorization)** on the TF-IDF matrix and interpret topics via top words.
- **Pretrained lexical embeddings (GloVe)**:
  - Download `glove.6B.100d` (zip), extract, load embeddings into memory.
  - Compute cosine similarity to find nearest neighbors for a sample term.
- **Subword/BPE embeddings**:
  - Use `tokenizers.BertWordPieceTokenizer` to build WordPiece/BPE-style tokens from the corpus.
  - Train Word2Vec on BPE tokens and inspect subword similarities.
- **Simple language-model-style embeddings**:
  - Build **unigram probability ‚Äúembeddings‚Äù** from token counts and compare similar-probability words.
- **Hybrid character+word embeddings**:
  - Create character n-gram vectors (`CountVectorizer(analyzer='char', ngram_range=(2,3))`)
  - Concatenate char n-grams with Word2Vec vectors to form hybrid representations.

### 03 ‚Äî Clustering & Sentiment Scoring (`03_clustering_and_sentiment.ipynb`)
**Goal**: Group posts by similarity and compute sentiment polarity scores.

**What I do**
- **Load preprocessing artifacts** produced by notebook `01`.
- **K-Means clustering** on the TF-IDF matrix (`sklearn.cluster.KMeans`) to assign cluster labels.
- **Cluster interpretation** by printing top TF-IDF terms per cluster.
- **Sentiment scoring** using **NLTK VADER** (`nltk.sentiment.vader.SentimentIntensityAnalyzer`) to generate polarity scores (best suited for English; used here as a baseline).

### 04 ‚Äî Supervised Sentiment Classification (`04_sentiment_classification.ipynb`)
**Goal**: Train and evaluate classical ML classifiers to predict `sentiment_label` from TF-IDF features.

**What I do**
- **Load preprocessing artifacts** produced by notebook `01`.
- **Train/test split** with stratification (`sklearn.model_selection.train_test_split`).
- Train and evaluate multiple baselines:
  - **Multinomial Na√Øve Bayes**
  - **Linear SVM (LinearSVC)**
  - **Decision Tree**
- Report **accuracy**, **classification report**, and **confusion matrix**.
- Optional: evaluate performance **per language**.

### 05 ‚Äî NER, Aspect/Topic Sentiment, Temporal Trends (`05_ner_aspect_temporal.ipynb`)
**Goal**: Extract entities, summarize sentiment by topic, and track sentiment over time.

**What I do**
- **Load preprocessing artifacts** produced by notebook `01`.
- **Transformer-based NER** (multilingual):
  - Use `transformers.pipeline("ner")` with `Davlan/xlm-roberta-base-ner-hrl` and aggregation to extract entities.
- **Simple topic/aspect sentiment view**:
  - For each `topic`, summarize the sentiment label distribution and show example posts.
- **Temporal sentiment analysis**:
  - Convert `timestamp` to datetime and map sentiment labels to numeric scores.
  - Aggregate sentiment over time and visualize trends with `matplotlib`.
  - Flag potential ‚Äúchange points‚Äù using a simple mean ¬± std threshold heuristic.

### 06 ‚Äî Summarization (Extractive + Abstractive) (`06_summarization.ipynb`)
**Goal**: Produce concise summaries of public discourse per topic.

**What I do**
- **Load preprocessing artifacts** produced by notebook `01`.
- **Extractive summarization** (English) using TextRank-style summarization from `summa`.
- **Abstractive summarization** (English) using `transformers.pipeline("summarization")` with `facebook/bart-large-cnn`.

## üß∞ Libraries Used (Across Notebooks)
Below is the combined set of libraries actually imported / installed in notebooks `01`‚Äì`06`.

### Core Python / utilities
- `pathlib`, `pickle`, `collections`, `os`, `re`, `unicodedata`, `urllib.request`, `zipfile`

### Data handling & math
- `pandas`, `numpy`

### NLP (classical)
- `nltk` (stopwords, VADER sentiment)
- `gensim` (Word2Vec, FastText)
- `tokenizers` (WordPiece/BPE tokenizer)

### ML / feature engineering
- `scikit-learn` (`CountVectorizer`, `TfidfVectorizer`, `NMF`, `KMeans`, `train_test_split`, classifiers, metrics, cosine similarity)

### Deep NLP / transformers
- `transformers` (pipelines for NER and summarization)
- `sentencepiece` (tokenization dependency used with transformers)

### Visualization & graphs
- `matplotlib`
- `networkx`

### Summarization
- `summa` (TextRank-style extractive summarization)

### Execution environment (notebook convenience)
- `google.colab` (Drive mounting cells in several notebooks)

## ‚úÖ Recommended Run Order
1. `01_data_loading_and_preprocessing.ipynb` (creates `artifacts/preprocessing_outputs.pkl`)
2. `02_embeddings_and_topic_modeling.ipynb`
3. `03_clustering_and_sentiment.ipynb`
4. `04_sentiment_classification.ipynb`
5. `05_ner_aspect_temporal.ipynb`
6. `06_summarization.ipynb`
