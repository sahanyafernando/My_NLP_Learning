{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahanyafernando/My_NLP_Learning/blob/main/Public_Response_Analysis/notebooks/05_ner_aspect_temporal.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 05 â€“ Entity Extraction, Aspect-Based Sentiment, and Temporal Analysis\n",
        "\n",
        "This notebook performs transformer-based NER, simple aspect-based sentiment analysis\n",
        "around policy topics, and temporal sentiment trend exploration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle, pathlib\n",
        "\n",
        "artifacts_root = pathlib.Path(\"/content/drive/MyDrive/My_NLP_Learning/Public_Response_Analysis\")\n",
        "artifacts_path = artifacts_root / \"artifacts/preprocessing_outputs.pkl\"\n",
        "\n",
        "if artifacts_path.exists():\n",
        "    with open(artifacts_path, \"rb\") as f:\n",
        "        artifacts = pickle.load(f)\n",
        "    df = artifacts[\"df\"]\n",
        "    print(\"Loaded preprocessing artifacts and DataFrame.\")\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        \"Artifacts not found. Please run 01_data_loading_and_preprocessing.ipynb first \"\n",
        "        \"and execute the 'Save preprocessing artifacts' cell.\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Transformer-based Named Entity Recognition (NER)\n",
        "\n",
        "We use a multilingual transformer model for NER to extract entities mentioned in posts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q transformers sentencepiece\n",
        "\n",
        "from transformers import pipeline\n",
        "\n",
        "ner_pipeline = pipeline(\n",
        "    task=\"ner\",\n",
        "    model=\"Davlan/xlm-roberta-base-ner-hrl\",\n",
        "    aggregation_strategy=\"simple\",\n",
        ")\n",
        "\n",
        "sample_texts = df[\"text\"].head(5).tolist()\n",
        "for text in sample_texts:\n",
        "    print(\"\\nText:\", text)\n",
        "    ents = ner_pipeline(text)\n",
        "    print(\"Entities:\", ents)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Aspect-based sentiment around policy topics\n",
        "\n",
        "We use the existing sentiment labels and topics as a simple ABSA setting:\n",
        "for each `topic`, we analyze the distribution of sentiment and key example posts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "topic_sent = (\n",
        "    df.groupby([\"topic\", \"sentiment_label\"])\n",
        "    .size()\n",
        "    .unstack(fill_value=0)\n",
        "    .sort_index()\n",
        ")\n",
        "\n",
        "print(\"Sentiment distribution per topic:\")\n",
        "print(topic_sent)\n",
        "\n",
        "for topic in df[\"topic\"].unique():\n",
        "    print(f\"\\n=== Topic: {topic} ===\")\n",
        "    subset = df[df[\"topic\"] == topic]\n",
        "    print(\"Example positive posts:\")\n",
        "    print(subset[subset[\"sentiment_label\"] == \"positive\"][\"text\"].head(3).to_string(index=False))\n",
        "    print(\"\\nExample negative posts:\")\n",
        "    print(subset[subset[\"sentiment_label\"] == \"negative\"][\"text\"].head(3).to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Temporal sentiment trends\n",
        "\n",
        "We convert timestamps to datetime, aggregate sentiment over time,\n",
        "and visualize changes across events and languages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "df_time = df.copy()\n",
        "df_time[\"timestamp\"] = pd.to_datetime(df_time[\"timestamp\"])\n",
        "\n",
        "sentiment_map = {\"negative\": -1, \"neutral\": 0, \"positive\": 1}\n",
        "df_time[\"sentiment_score\"] = df_time[\"sentiment_label\"].map(sentiment_map)\n",
        "\n",
        "daily = df_time.set_index(\"timestamp\").groupby([pd.Grouper(freq=\"D\")])[\"sentiment_score\"].mean()\n",
        "\n",
        "plt.figure(figsize=(10, 4))\n",
        "daily.plot(marker=\"o\")\n",
        "plt.title(\"Average sentiment over time\")\n",
        "plt.xlabel(\"Date\")\n",
        "plt.ylabel(\"Average sentiment score\")\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Simple change-point style analysis\n",
        "\n",
        "We flag days where the sentiment deviates strongly from the overall mean\n",
        "as potential change-points related to key events."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "mean_sent = daily.mean()\n",
        "std_sent = daily.std()\n",
        "threshold = mean_sent + 1.0 * std_sent\n",
        "\n",
        "print(\"Global mean sentiment:\", mean_sent)\n",
        "print(\"Global std sentiment:\", std_sent)\n",
        "\n",
        "anomalies = daily[daily > threshold]\n",
        "print(\"\\nPotential positive sentiment spikes:\")\n",
        "print(anomalies)\n",
        "\n",
        "threshold_neg = mean_sent - 1.0 * std_sent\n",
        "anomalies_neg = daily[daily < threshold_neg]\n",
        "print(\"\\nPotential negative sentiment drops:\")\n",
        "print(anomalies_neg)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyNlbEDktmhmgYLf9YCV02x/",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
