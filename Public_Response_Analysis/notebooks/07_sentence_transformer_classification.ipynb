{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahanyafernando/My_NLP_Learning/blob/main/Public_Response_Analysis/notebooks/07_sentence_transformer_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 07 â€“ Sentence Transformer-Based Text Classification\n",
        "\n",
        "This notebook uses **Sentence Transformers** to create dense embeddings and performs sentiment classification on the multilingual policy dataset. Unlike the classical TF-IDF approach in notebook 04, this uses state-of-the-art transformer-based embeddings that capture semantic meaning across languages."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load preprocessing artifacts\n",
        "\n",
        "Loads outputs saved by `01_data_loading_and_preprocessing.ipynb`. Run that notebook first if this file is missing."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle, pathlib\n",
        "\n",
        "artifacts_root = pathlib.Path(\"/content/drive/MyDrive/My_NLP_Learning/Public_Response_Analysis\")\n",
        "artifacts_path = artifacts_root / \"artifacts/preprocessing_outputs.pkl\"\n",
        "\n",
        "if artifacts_path.exists():\n",
        "    with open(artifacts_path, \"rb\") as f:\n",
        "        artifacts = pickle.load(f)\n",
        "    df = artifacts[\"df\"]\n",
        "    print(\"Loaded preprocessing artifacts and DataFrame.\")\n",
        "    print(f\"Dataset shape: {df.shape}\")\n",
        "    print(f\"Sentiment labels: {df['sentiment_label'].value_counts().to_dict()}\")\n",
        "else:\n",
        "    raise FileNotFoundError(\n",
        "        \"Artifacts not found. Please run 01_data_loading_and_preprocessing.ipynb first \"\n",
        "        \"and execute the 'Save preprocessing artifacts' cell.\"\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install and import Sentence Transformers\n",
        "\n",
        "We'll use the `sentence-transformers` library to create multilingual embeddings. This library provides pre-trained models optimized for creating sentence embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Generate Sentence Embeddings\n",
        "\n",
        "We'll use a multilingual Sentence Transformer model to convert each text post into a dense vector representation. The model `paraphrase-multilingual-MiniLM-L12-v2` supports 50+ languages and creates 384-dimensional embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Initialize the multilingual sentence transformer model\n",
        "# This model supports 50+ languages including en, es, fr, de, hi (languages in our dataset)\n",
        "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
        "\n",
        "print(\"Sentence Transformer model loaded successfully!\")\n",
        "print(f\"Model embedding dimension: {model.get_sentence_embedding_dimension()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate embeddings for all texts\n",
        "print(\"Generating sentence embeddings...\")\n",
        "texts = df['text'].tolist()\n",
        "embeddings = model.encode(texts, show_progress_bar=True, batch_size=16)\n",
        "\n",
        "print(f\"Generated embeddings shape: {embeddings.shape}\")\n",
        "print(f\"Sample embedding (first 10 dimensions): {embeddings[0][:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prepare Training Data\n",
        "\n",
        "Split the data into training and testing sets using the sentiment labels as targets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = embeddings\n",
        "y = df['sentiment_label'].values\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "print(f\"Training set size: {X_train.shape[0]}\")\n",
        "print(f\"Test set size: {X_test.shape[0]}\")\n",
        "print(f\"\\nTraining label distribution:\")\n",
        "print(pd.Series(y_train).value_counts())\n",
        "print(f\"\\nTest label distribution:\")\n",
        "print(pd.Series(y_test).value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Classification Models\n",
        "\n",
        "We'll train multiple classifiers on the sentence transformer embeddings and compare their performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define models to train\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
        "    \"SVM (Linear)\": SVC(kernel='linear', random_state=42, probability=True),\n",
        "    \"SVM (RBF)\": SVC(kernel='rbf', random_state=42, probability=True),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, clf in models.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {name}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "    \n",
        "    # Train the model\n",
        "    clf.fit(X_train, y_train)\n",
        "    \n",
        "    # Make predictions\n",
        "    y_pred = clf.predict(X_test)\n",
        "    \n",
        "    # Calculate accuracy\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    results[name] = acc\n",
        "    \n",
        "    print(f\"\\n{name} Accuracy: {acc:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(y_test, y_pred))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"Summary of Model Accuracies:\")\n",
        "print(f\"{'='*60}\")\n",
        "for name, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
        "    print(f\"{name}: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Language-wise Performance Analysis\n",
        "\n",
        "Evaluate how well the best model performs across different languages in the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Select the best model\n",
        "best_model_name = max(results, key=results.get)\n",
        "print(f\"Best model: {best_model_name}\")\n",
        "best_clf = models[best_model_name]\n",
        "\n",
        "# Make predictions on entire dataset for language-wise analysis\n",
        "y_pred_all = best_clf.predict(X)\n",
        "\n",
        "df_eval = df.copy()\n",
        "df_eval['predicted_sentiment'] = y_pred_all\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Language-wise Performance:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for lang in sorted(df_eval['language'].unique()):\n",
        "    lang_subset = df_eval[df_eval['language'] == lang]\n",
        "    lang_acc = accuracy_score(lang_subset['sentiment_label'], lang_subset['predicted_sentiment'])\n",
        "    print(f\"\\nLanguage: {lang.upper()}\")\n",
        "    print(f\"  Accuracy: {lang_acc:.4f}\")\n",
        "    print(f\"  Number of samples: {len(lang_subset)}\")\n",
        "    print(f\"  Classification Report:\")\n",
        "    print(classification_report(lang_subset['sentiment_label'], lang_subset['predicted_sentiment'], \n",
        "                                zero_division=0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Comparison with TF-IDF Baseline (Notebook 04)\n",
        "\n",
        "This cell demonstrates the advantage of sentence transformers over TF-IDF for multilingual text classification. If you ran notebook 04, you can compare the results.\n",
        "\n",
        "**Key advantages of Sentence Transformers:**\n",
        "- **Semantic understanding**: Captures meaning, not just word frequencies\n",
        "- **Multilingual**: Single model handles multiple languages seamlessly\n",
        "- **Context-aware**: Understands context and word relationships\n",
        "- **Dense embeddings**: Compact 384-dimensional vectors vs sparse TF-IDF matrices"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
