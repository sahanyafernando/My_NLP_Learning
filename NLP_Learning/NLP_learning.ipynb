{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahanyafernando/My_NLP_Learning/blob/main/NLP_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstration: Lowercasing, Stopword Removal, Tokenization and One-Hot Encoding"
      ],
      "metadata": {
        "id": "XDbvP_b8X4jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Part 01: Lowercasing, Stopword Removal and Tokenization"
      ],
      "metadata": {
        "id": "mPSQm_EqYg5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"text_classification.csv\")\n",
        "print(\"Original Dataset:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "lAvlzd_fw9Ls",
        "outputId": "788259ae-f68d-4003-cfe8-8313424625da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'text_classification.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2001700855.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_classification.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Original Dataset:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'text_classification.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "cTWSWQLU52Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This outputs one colunm with its lower case form\n",
        "df[\"Lowercase_Pandas\"] = df[\"text\"].str.lower()\n",
        "print(df[[\"Lowercase_Pandas\"]].head())\n"
      ],
      "metadata": {
        "id": "QyiCOGHD6baN",
        "outputId": "984ea113-4b04-411d-c9f6-6cf15de6cbff",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            Lowercase_Pandas\n",
            "0  the stock market rose by 300 points today\n",
            "1   manchester united won the football match\n",
            "2  new research shows benefits of meditation\n",
            "3            the government passed a new law\n",
            "4      scientists discovered a new exoplanet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# if we want to get the 2 colunms with their lowercase form then\n",
        "df[\"text_lower\"] = df[\"text\"].str.lower()\n",
        "df[\"category_lower\"] = df[\"category\"].str.lower()\n",
        "print(df[[\"text_lower\", \"category_lower\"]].head())"
      ],
      "metadata": {
        "id": "05yefqak8oXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "146a9fa0-a003-46b8-a6b2-268f98ed9878"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  text_lower category_lower\n",
            "0  the stock market rose by 300 points today        finance\n",
            "1   manchester united won the football match         sports\n",
            "2  new research shows benefits of meditation         health\n",
            "3            the government passed a new law       politics\n",
            "4      scientists discovered a new exoplanet        science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using spaCy\n",
        "df = pd.read_csv(\"faq_dataset.csv\")\n",
        "\n",
        "def lowercase_spacy(text):\n",
        "  tokens = [token.text.lower() for token in nlp(text)]\n",
        "  return \" \".join(tokens)    # if we want we can add separate character like \" \"\n",
        "df[\"Lowercase_spaCy\"] = df[\"question\"].apply(lowercase_spacy)\n",
        "print(df[[\"Lowercase_spaCy\"]].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dXHccJo_uld",
        "outputId": "2f2c7331-6508-4799-cc26-11a03b9d0af0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Lowercase_spaCy\n",
            "0                  what is nlp ?\n",
            "1         who developed python ?\n",
            "2     what is a neural network ?\n",
            "3   what is sentiment analysis ?\n",
            "4  what is tokenization in nlp ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_word_nltk = set(stopwords.words(\"english\"))\n",
        "stop_words_spacy = nlp.Defaults.stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNuUFjEgG6n9",
        "outputId": "a5a0453d-4d06-4b2d-91c2-04b780d9699a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV\n",
        "df = pd.read_csv(\"text_classification.csv\")\n",
        "\n",
        "# Convert 'text' column to lowercase\n",
        "df[\"text_lower\"] = df[\"text\"].str.lower()\n",
        "\n",
        "# Using NLTK\n",
        "def remove_stopwords_nltk(text):\n",
        "  words = word_tokenize(text)\n",
        "  return \" \".join([word for word in words if word not in stop_word_nltk])\n",
        "df[\"No_Stopwords_NLTK\"] = df[\"text_lower\"].apply(remove_stopwords_nltk)\n",
        "print(df[[\"No_Stopwords_NLTK\"]].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFf5mRvlI1R1",
        "outputId": "addd6aaa-0794-4ba4-89c3-c46afc090c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        No_Stopwords_NLTK\n",
            "0      stock market rose 300 points today\n",
            "1        manchester united football match\n",
            "2  new research shows benefits meditation\n",
            "3               government passed new law\n",
            "4     scientists discovered new exoplanet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**One Hot Encoding**"
      ],
      "metadata": {
        "id": "EK5rkK7L624k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "YipVgZj16-MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using NLTK  tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "df[\"Tokenized_NLTK\"] = df[\"text_lower\"].apply(word_tokenize)\n",
        "print(df[\"Tokenized_NLTK\"].head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiRp1QZv7e6x",
        "outputId": "7baf0d96-1349-4e46-cb5b-1fcbc4b73686"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [the, stock, market, rose, by, 300, points, to...\n",
            "1      [manchester, united, won, the, football, match]\n",
            "2     [new, research, shows, benefits, of, meditation]\n",
            "3               [the, government, passed, a, new, law]\n",
            "4          [scientists, discovered, a, new, exoplanet]\n",
            "Name: Tokenized_NLTK, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "df_onehot_nltk = pd.DataFrame(mlb.fit_transform(df[\"Tokenized_NLTK\"]), columns = mlb.classes_)\n",
        "print(\"- Using NLTK:\")\n",
        "print(df_onehot_nltk.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfZNb6GH9cDD",
        "outputId": "ba663ab6-b349-439e-93f4-8a7f1cd96de2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Using NLTK:\n",
            "   300  a  benefits  by  discovered  exoplanet  football  government  law  \\\n",
            "0    1  0         0   1           0          0         0           0    0   \n",
            "1    0  0         0   0           0          0         1           0    0   \n",
            "2    0  0         1   0           0          0         0           0    0   \n",
            "3    0  1         0   0           0          0         0           1    1   \n",
            "4    0  1         0   0           1          1         0           0    0   \n",
            "\n",
            "   manchester  ...  points  research  rose  scientists  shows  stock  the  \\\n",
            "0           0  ...       1         0     1           0      0      1    1   \n",
            "1           1  ...       0         0     0           0      0      0    1   \n",
            "2           0  ...       0         1     0           0      1      0    0   \n",
            "3           0  ...       0         0     0           0      0      0    1   \n",
            "4           0  ...       0         0     0           1      0      0    0   \n",
            "\n",
            "   today  united  won  \n",
            "0      1       0    0  \n",
            "1      0       1    1  \n",
            "2      0       0    0  \n",
            "3      0       0    0  \n",
            "4      0       0    0  \n",
            "\n",
            "[5 rows x 26 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Scikit-learn\n",
        "vectorizer = CountVectorizer()\n",
        "df_onehot_sklearn = pd.DataFrame(vectorizer.fit_transform(df[\"No_Stopwords_NLTK\"]).toarray(), columns = vectorizer.get_feature_names_out())\n",
        "print(\"- Using Scikit-learn:\")\n",
        "print(df_onehot_sklearn.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiMnd_3I96ET",
        "outputId": "b57c2127-0d47-493e-e898-0ec9e339c52e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Using Scikit-learn:\n",
            "   300  benefits  discovered  exoplanet  football  government  law  \\\n",
            "0    1         0           0          0         0           0    0   \n",
            "1    0         0           0          0         1           0    0   \n",
            "2    0         1           0          0         0           0    0   \n",
            "3    0         0           0          0         0           1    1   \n",
            "4    0         0           1          1         0           0    0   \n",
            "\n",
            "   manchester  market  match  ...  new  passed  points  research  rose  \\\n",
            "0           0       1      0  ...    0       0       1         0     1   \n",
            "1           1       0      1  ...    0       0       0         0     0   \n",
            "2           0       0      0  ...    1       0       0         1     0   \n",
            "3           0       0      0  ...    1       1       0         0     0   \n",
            "4           0       0      0  ...    1       0       0         0     0   \n",
            "\n",
            "   scientists  shows  stock  today  united  \n",
            "0           0      0      1      1       0  \n",
            "1           0      0      0      0       1  \n",
            "2           0      1      0      0       0  \n",
            "3           0      0      0      0       0  \n",
            "4           1      0      0      0       0  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"processed_dataset.csv\", index = False)\n",
        "print(\"Preprocessing complete. Saved as processed_dataset.csv.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0-Xpt0oGuPm",
        "outputId": "66e9ab82-be30-4581-b535-affa105d328c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete. Saved as processed_dataset.csv.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}