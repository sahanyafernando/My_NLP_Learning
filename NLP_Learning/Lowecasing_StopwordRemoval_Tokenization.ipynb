{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahanyafernando/My_NLP_Learning/blob/main/NLP_Learning/Lowecasing_StopwordRemoval_Tokenization.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demonstration: Lowercasing, Stopword Removal, Tokenization and One-Hot Encoding"
      ],
      "metadata": {
        "id": "XDbvP_b8X4jv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Lowercasing, Stopword Removal and Tokenization"
      ],
      "metadata": {
        "id": "mPSQm_EqYg5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# Load dataset\n",
        "df = pd.read_csv(\"text_classification.csv\")\n",
        "print(\"Original Dataset:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "lAvlzd_fw9Ls",
        "outputId": "40d82b8d-0f73-486a-f642-315d383bb986",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "                                        text  category\n",
            "0  The stock market rose by 300 points today   finance\n",
            "1   Manchester United won the football match    sports\n",
            "2  New research shows benefits of meditation    health\n",
            "3            The government passed a new law  politics\n",
            "4      Scientists discovered a new exoplanet   science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "metadata": {
        "id": "cTWSWQLU52Pr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lowercasing"
      ],
      "metadata": {
        "id": "dywAYATwpQ_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# This outputs one colunm with its lower case form\n",
        "df[\"Lowercase_Pandas\"] = df[\"text\"].str.lower()\n",
        "print(df[[\"Lowercase_Pandas\"]].head())\n"
      ],
      "metadata": {
        "id": "QyiCOGHD6baN",
        "outputId": "84e9f522-c36b-43ab-c4b8-ae84373571fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            Lowercase_Pandas\n",
            "0  the stock market rose by 300 points today\n",
            "1   manchester united won the football match\n",
            "2  new research shows benefits of meditation\n",
            "3            the government passed a new law\n",
            "4      scientists discovered a new exoplanet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# if we want to get the 2 colunms with their lowercase form then\n",
        "df[\"text_lower\"] = df[\"text\"].str.lower()\n",
        "df[\"category_lower\"] = df[\"category\"].str.lower()\n",
        "print(df[[\"text_lower\", \"category_lower\"]].head())"
      ],
      "metadata": {
        "id": "05yefqak8oXD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d658fed-6616-4d82-efe9-9959dffe8e73"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                  text_lower category_lower\n",
            "0  the stock market rose by 300 points today        finance\n",
            "1   manchester united won the football match         sports\n",
            "2  new research shows benefits of meditation         health\n",
            "3            the government passed a new law       politics\n",
            "4      scientists discovered a new exoplanet        science\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using spaCy\n",
        "df = pd.read_csv(\"faq_dataset.csv\")\n",
        "\n",
        "def lowercase_spacy(text):\n",
        "  tokens = [token.text.lower() for token in nlp(text)]\n",
        "  return \" \".join(tokens)    # if we want we can add separate character like \" \"\n",
        "df[\"Lowercase_spaCy\"] = df[\"question\"].apply(lowercase_spacy)\n",
        "print(df[[\"Lowercase_spaCy\"]].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8dXHccJo_uld",
        "outputId": "ace526f9-5cb9-4c64-8976-939f148bd9c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                 Lowercase_spaCy\n",
            "0                  what is nlp ?\n",
            "1         who developed python ?\n",
            "2     what is a neural network ?\n",
            "3   what is sentiment analysis ?\n",
            "4  what is tokenization in nlp ?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Stop Word Removal"
      ],
      "metadata": {
        "id": "15K-loBMpZnc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download(\"stopwords\")\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download(\"punkt_tab\")\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "stop_word_nltk = set(stopwords.words(\"english\"))\n",
        "stop_words_spacy = nlp.Defaults.stop_words"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZNuUFjEgG6n9",
        "outputId": "265204c1-c7e7-4cbd-dd0d-7e784e492061"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load CSV\n",
        "df = pd.read_csv(\"text_classification.csv\")\n",
        "\n",
        "# Convert 'text' column to lowercase\n",
        "df[\"text_lower\"] = df[\"text\"].str.lower()\n",
        "\n",
        "# Using NLTK\n",
        "def remove_stopwords_nltk(text):\n",
        "  words = word_tokenize(text)\n",
        "  return \" \".join([word for word in words if word not in stop_word_nltk])\n",
        "df[\"No_Stopwords_NLTK\"] = df[\"text_lower\"].apply(remove_stopwords_nltk)\n",
        "print(df[[\"No_Stopwords_NLTK\"]].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFf5mRvlI1R1",
        "outputId": "82fd33ff-15fa-44fd-eebe-3965d4d9de1b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        No_Stopwords_NLTK\n",
            "0      stock market rose 300 points today\n",
            "1        manchester united football match\n",
            "2  new research shows benefits meditation\n",
            "3               government passed new law\n",
            "4     scientists discovered new exoplanet\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### One Hot Encoding"
      ],
      "metadata": {
        "id": "EK5rkK7L624k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MultiLabelBinarizer\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "YipVgZj16-MK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Using NLTK  tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "df[\"Tokenized_NLTK\"] = df[\"text_lower\"].apply(word_tokenize)\n",
        "print(df[\"Tokenized_NLTK\"].head())\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiRp1QZv7e6x",
        "outputId": "77aef73f-0f08-4f0d-cd1d-5620ef22c84b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0    [the, stock, market, rose, by, 300, points, to...\n",
            "1      [manchester, united, won, the, football, match]\n",
            "2     [new, research, shows, benefits, of, meditation]\n",
            "3               [the, government, passed, a, new, law]\n",
            "4          [scientists, discovered, a, new, exoplanet]\n",
            "Name: Tokenized_NLTK, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mlb = MultiLabelBinarizer()\n",
        "df_onehot_nltk = pd.DataFrame(mlb.fit_transform(df[\"Tokenized_NLTK\"]), columns = mlb.classes_)\n",
        "print(\"- Using NLTK:\")\n",
        "print(df_onehot_nltk.head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfZNb6GH9cDD",
        "outputId": "4b1e9962-6787-4bc9-a45d-0cb54eb021f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Using NLTK:\n",
            "   300  a  benefits  by  discovered  exoplanet  football  government  law  \\\n",
            "0    1  0         0   1           0          0         0           0    0   \n",
            "1    0  0         0   0           0          0         1           0    0   \n",
            "2    0  0         1   0           0          0         0           0    0   \n",
            "3    0  1         0   0           0          0         0           1    1   \n",
            "4    0  1         0   0           1          1         0           0    0   \n",
            "\n",
            "   manchester  ...  points  research  rose  scientists  shows  stock  the  \\\n",
            "0           0  ...       1         0     1           0      0      1    1   \n",
            "1           1  ...       0         0     0           0      0      0    1   \n",
            "2           0  ...       0         1     0           0      1      0    0   \n",
            "3           0  ...       0         0     0           0      0      0    1   \n",
            "4           0  ...       0         0     0           1      0      0    0   \n",
            "\n",
            "   today  united  won  \n",
            "0      1       0    0  \n",
            "1      0       1    1  \n",
            "2      0       0    0  \n",
            "3      0       0    0  \n",
            "4      0       0    0  \n",
            "\n",
            "[5 rows x 26 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Using Scikit-learn\n",
        "vectorizer = CountVectorizer()\n",
        "df_onehot_sklearn = pd.DataFrame(vectorizer.fit_transform(df[\"No_Stopwords_NLTK\"]).toarray(), columns = vectorizer.get_feature_names_out())\n",
        "print(\"- Using Scikit-learn:\")\n",
        "print(df_onehot_sklearn.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NiMnd_3I96ET",
        "outputId": "b8ed6878-05e4-483b-f794-a3da75093e38"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "- Using Scikit-learn:\n",
            "   300  benefits  discovered  exoplanet  football  government  law  \\\n",
            "0    1         0           0          0         0           0    0   \n",
            "1    0         0           0          0         1           0    0   \n",
            "2    0         1           0          0         0           0    0   \n",
            "3    0         0           0          0         0           1    1   \n",
            "4    0         0           1          1         0           0    0   \n",
            "\n",
            "   manchester  market  match  ...  new  passed  points  research  rose  \\\n",
            "0           0       1      0  ...    0       0       1         0     1   \n",
            "1           1       0      1  ...    0       0       0         0     0   \n",
            "2           0       0      0  ...    1       0       0         1     0   \n",
            "3           0       0      0  ...    1       1       0         0     0   \n",
            "4           0       0      0  ...    1       0       0         0     0   \n",
            "\n",
            "   scientists  shows  stock  today  united  \n",
            "0           0      0      1      1       0  \n",
            "1           0      0      0      0       1  \n",
            "2           0      1      0      0       0  \n",
            "3           0      0      0      0       0  \n",
            "4           1      0      0      0       0  \n",
            "\n",
            "[5 rows x 21 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"processed_dataset.csv\", index = False)\n",
        "print(\"Preprocessing complete. Saved as processed_dataset.csv.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0-Xpt0oGuPm",
        "outputId": "26a25107-7dad-43ec-960d-56b24aa8f408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preprocessing complete. Saved as processed_dataset.csv.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demonstration : Word Tokenization"
      ],
      "metadata": {
        "id": "AfDFKH3FccFA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # Import pandas for handling CSV data\n",
        "import nltk # Import NLTK for natural language processing\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize # Import tokenization functions from nltk\n",
        "from transformers import BertTokenizer # Import BERT tokenizer for subword tokenization\n",
        "\n",
        "# Download necessary resources for NLTK tokenization\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "9-dwb-stdTwx",
        "outputId": "29f883d9-8bc1-4c8f-c1d3-c96f3f18395e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load dataset from CSV file\n",
        "file_path = \"faq_dataset.csv\"\n",
        "df = pd.read_csv(file_path) # Read CSV file into a DataFrame"
      ],
      "metadata": {
        "id": "TqMZWxesfFb_"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure 'faq_Review' column exists in the dataset\n",
        "if 'faq_Review' not in df.columns:\n",
        "    raise ValueError(\"Error: 'faq_Review' column not found in the dataset.\")\n"
      ],
      "metadata": {
        "id": "k2U82mcIfw_7",
        "outputId": "6b855a11-c935-43bc-8cfa-aef7a786b982",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Error: 'faq_Review' column not found in the dataset.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2821100634.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Ensure 'faq_Review' column exists in the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;34m'faq_Review'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Error: 'faq_Review' column not found in the dataset.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m: Error: 'faq_Review' column not found in the dataset."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display first few rows of the dataset to understand its structure\n",
        "print(\"Original Dataset:\")\n",
        "print(df.head())"
      ],
      "metadata": {
        "id": "KitEwWYngzv2",
        "outputId": "9dff0024-24fc-4aaa-9a1c-839e166c8c7c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Dataset:\n",
            "                       question  \\\n",
            "0                  What is NLP?   \n",
            "1         Who developed Python?   \n",
            "2     What is a neural network?   \n",
            "3   What is sentiment analysis?   \n",
            "4  What is tokenization in NLP?   \n",
            "\n",
            "                                              answer  \n",
            "0  Natural Language Processing is a field of AI t...  \n",
            "1          Python was developed by Guido van Rossum.  \n",
            "2  A neural network is a set of algorithms modele...  \n",
            "3  Sentiment analysis is the process of determini...  \n",
            "4  Tokenization is splitting text into words, phr...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# WORD TOKENIZATION\n",
        "print(\"\\nWord Tokenization:\")\n",
        "df['Word_Tokens'] = df['question'].apply(word_tokenize) # Apply word tokenization\n",
        "print(df[['question', 'Word_Tokens']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q6V3hnjZfift",
        "outputId": "065c6c67-a263-4751-8c45-d9ad4491bf17"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Word Tokenization:\n",
            "                       question                           Word_Tokens\n",
            "0                  What is NLP?                    [What, is, NLP, ?]\n",
            "1         Who developed Python?           [Who, developed, Python, ?]\n",
            "2     What is a neural network?     [What, is, a, neural, network, ?]\n",
            "3   What is sentiment analysis?    [What, is, sentiment, analysis, ?]\n",
            "4  What is tokenization in NLP?  [What, is, tokenization, in, NLP, ?]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SUBWORD TOKENIZATION (BERT Tokenizer)\n",
        "print(\"\\nSubword Tokenization (BERT):\")\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # Load pre-trained BERT tokenizer\n",
        "df['Subword_Tokens'] = df['question'].apply(lambda x: tokenizer.tokenize(x)) # Apply subword tokenization\n",
        "print(df[['question', 'Subword_Tokens']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zS_HSPrVgUKG",
        "outputId": "75d728e2-f08e-4be3-f373-0306636ad79b"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Subword Tokenization (BERT):\n",
            "                       question                                Subword_Tokens\n",
            "0                  What is NLP?                        [what, is, nl, ##p, ?]\n",
            "1         Who developed Python?                   [who, developed, python, ?]\n",
            "2     What is a neural network?             [what, is, a, neural, network, ?]\n",
            "3   What is sentiment analysis?            [what, is, sentiment, analysis, ?]\n",
            "4  What is tokenization in NLP?  [what, is, token, ##ization, in, nl, ##p, ?]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# SENTENCE TOKENIZATION\n",
        "print(\"\\nSentence Tokenization:\")\n",
        "df['sent_tokenize'] = df['question'].apply(sent_tokenize) # Apply sentence tokenization\n",
        "print(df[['question', 'sent_tokenize']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2ob7jVxRh6Se",
        "outputId": "b0a902c1-8b48-4116-8c77-57ea3c992247"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sentence Tokenization:\n",
            "                       question                   sent_tokenize\n",
            "0                  What is NLP?                  [What is NLP?]\n",
            "1         Who developed Python?         [Who developed Python?]\n",
            "2     What is a neural network?     [What is a neural network?]\n",
            "3   What is sentiment analysis?   [What is sentiment analysis?]\n",
            "4  What is tokenization in NLP?  [What is tokenization in NLP?]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# CHARACTER TOKENIZATION\n",
        "print(\"\\nCharacter Tokenization:\")\n",
        "df['character_tokens'] = df['question'].apply(list)\n",
        "print(df[['question', 'character_tokens']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uG7hBQ-WjLbE",
        "outputId": "d23b7b34-6adb-4359-a229-d5806b88fa5c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Character Tokenization:\n",
            "                       question  \\\n",
            "0                  What is NLP?   \n",
            "1         Who developed Python?   \n",
            "2     What is a neural network?   \n",
            "3   What is sentiment analysis?   \n",
            "4  What is tokenization in NLP?   \n",
            "\n",
            "                                    character_tokens  \n",
            "0               [W, h, a, t,  , i, s,  , N, L, P, ?]  \n",
            "1  [W, h, o,  , d, e, v, e, l, o, p, e, d,  , P, ...  \n",
            "2  [W, h, a, t,  , i, s,  , a,  , n, e, u, r, a, ...  \n",
            "3  [W, h, a, t,  , i, s,  , s, e, n, t, i, m, e, ...  \n",
            "4  [W, h, a, t,  , i, s,  , t, o, k, e, n, i, z, ...  \n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}