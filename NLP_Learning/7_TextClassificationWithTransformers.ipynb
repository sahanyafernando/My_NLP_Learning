{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYq5KXmYRyv7"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahanyafernando/My_NLP_Learning/blob/main/NLP_Learning/7_TextClassificationWithTransformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRXEec9JRyv-"
      },
      "source": [
        "# Demonstration: Text Classification using Transformers (Sentence Transformers)\n",
        "\n",
        "This notebook demonstrates text classification using **Sentence Transformers** for creating semantic embeddings, which are then used to train classification models. This approach captures semantic meaning better than traditional TF-IDF methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hv9la9JARyv_"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bDaauuBdRywA"
      },
      "source": [
        "### Step 1: Loading the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VTQtfC-WRywA",
        "outputId": "31e3d70a-3d1c-4f93-c533-53e50b85323b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset Loaded:\n",
            "                                        review sentiment\n",
            "9           I am worried about the second wave  Negative\n",
            "11           Recovered patients are increasing  Positive\n",
            "0         I loved the movie, it was fantastic!  positive\n",
            "13  Mask wearing is mandatory in public places   Neutral\n",
            "5               Covid cases are rising rapidly  Negative\n"
          ]
        }
      ],
      "source": [
        "train_df = pd.read_csv(\"sentiment_analysis.csv\", encoding='latin-1').sample(10, random_state=42)\n",
        "test_df = pd.read_csv(\"sentiment_analysis.csv\", encoding='latin-1')\n",
        "\n",
        "print(\"Dataset Loaded:\")\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErizUUezRywB"
      },
      "source": [
        "### Step 2: Selecting relevant columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHHD3ChgRywB",
        "outputId": "308fdadc-a5e2-4915-d6fc-6af33cb05388"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Selected columns:\n",
            "                                        review sentiment\n",
            "9           I am worried about the second wave  Negative\n",
            "11           Recovered patients are increasing  Positive\n",
            "0         I loved the movie, it was fantastic!  positive\n",
            "13  Mask wearing is mandatory in public places   Neutral\n",
            "5               Covid cases are rising rapidly  Negative\n"
          ]
        }
      ],
      "source": [
        "train_df = train_df[['review', 'sentiment']]\n",
        "test_df = test_df[['review', 'sentiment']]\n",
        "\n",
        "print(\"Selected columns:\")\n",
        "print(train_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdlH0j9iRywC"
      },
      "source": [
        "### Step 3: Standardizing labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q3eZxwtvRywC",
        "outputId": "6f539bfe-d476-4053-80ac-add061015330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Standardized labels:\n",
            "                                        review  sentiment\n",
            "9           I am worried about the second wave          0\n",
            "11           Recovered patients are increasing          1\n",
            "0         I loved the movie, it was fantastic!          2\n",
            "13  Mask wearing is mandatory in public places          2\n",
            "5               Covid cases are rising rapidly          0\n",
            "\n",
            "Label distribution:\n",
            "sentiment\n",
            "0    2\n",
            "1    3\n",
            "2    5\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "def standardize_sentiment(Sentiment):\n",
        "    if Sentiment in ['Positive', 'Extremely Positive']:\n",
        "        return 1\n",
        "    elif Sentiment in ['Negative', 'Extremely Negative']:\n",
        "        return 0\n",
        "    else:\n",
        "        return 2 # Neutral\n",
        "\n",
        "train_df['sentiment'] = train_df['sentiment'].apply(standardize_sentiment)\n",
        "test_df['sentiment'] = test_df['sentiment'].apply(standardize_sentiment)\n",
        "\n",
        "print(\"Standardized labels:\")\n",
        "print(train_df.head())\n",
        "print(\"\\nLabel distribution:\")\n",
        "print(train_df['sentiment'].value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nS5OdHdtRywC"
      },
      "source": [
        "### Step 4: Install Sentence Transformers Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "GzIpAkfsRywD"
      },
      "outputs": [],
      "source": [
        "!pip install -q sentence-transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rpmJSKMdRywD"
      },
      "source": [
        "### Step 5: Generate Sentence Embeddings using Transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RE3IGvTRRywD"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "# Load a pre-trained sentence transformer model\n",
        "# This model creates 384-dimensional embeddings that capture semantic meaning\n",
        "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "print(\"Sentence Transformer model loaded successfully!\")\n",
        "print(f\"Model embedding dimension: {model.get_sentence_embedding_dimension()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B-D4czqgRywD"
      },
      "outputs": [],
      "source": [
        "# Generate embeddings for training data\n",
        "print(\"Generating embeddings for training data...\")\n",
        "train_embeddings = model.encode(train_df['review'].tolist(), show_progress_bar=True, batch_size=16)\n",
        "\n",
        "# Generate embeddings for test data\n",
        "print(\"\\nGenerating embeddings for test data...\")\n",
        "test_embeddings = model.encode(test_df['review'].tolist(), show_progress_bar=True, batch_size=16)\n",
        "\n",
        "print(f\"\\nTraining embeddings shape: {train_embeddings.shape}\")\n",
        "print(f\"Test embeddings shape: {test_embeddings.shape}\")\n",
        "print(f\"\\nSample embedding (first 10 dimensions): {train_embeddings[0][:10]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2-olbLL6RywD"
      },
      "source": [
        "### Step 6: Prepare Training and Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g9y9lxTYRywD",
        "outputId": "91665a92-b84f-498d-c151-67818dcdbc9d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: (10, 384)\n",
            "Test set size: (15, 384)\n",
            "\n",
            "Training label distribution:\n",
            "0    2\n",
            "1    3\n",
            "2    5\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Test label distribution:\n",
            "0    3\n",
            "1    4\n",
            "2    8\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "X_train = train_embeddings\n",
        "X_test = test_embeddings\n",
        "Y_train = train_df['sentiment'].values\n",
        "Y_test = test_df['sentiment'].values\n",
        "\n",
        "print(f\"Training set size: {X_train.shape}\")\n",
        "print(f\"Test set size: {X_test.shape}\")\n",
        "print(f\"\\nTraining label distribution:\")\n",
        "print(pd.Series(Y_train).value_counts().sort_index())\n",
        "print(f\"\\nTest label distribution:\")\n",
        "print(pd.Series(Y_test).value_counts().sort_index())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elwqIta8RywD"
      },
      "source": [
        "### Step 7: Training Classification Models\n",
        "\n",
        "We'll train multiple classifiers on the transformer embeddings and compare their performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "U5ePXjzBRywE"
      },
      "outputs": [],
      "source": [
        "def train_and_evaluate(model, name, X_train, Y_train, X_test, Y_test):\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Training {name}...\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    model.fit(X_train, Y_train)\n",
        "    Y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(Y_test, Y_pred)\n",
        "\n",
        "    print(f\"\\n{name} Performance:\")\n",
        "    print(f\"Accuracy: {accuracy:.4f}\")\n",
        "    print(\"\\nClassification Report:\")\n",
        "    print(classification_report(Y_test, Y_pred, zero_division=0))\n",
        "    print(\"\\nConfusion Matrix:\")\n",
        "    print(confusion_matrix(Y_test, Y_pred))\n",
        "\n",
        "    return accuracy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qOIH-1hZRywE",
        "outputId": "168b9ba6-aea5-46f4-8bcd-60ee0a7e1a0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Training Logistic Regression...\n",
            "============================================================\n",
            "\n",
            "Logistic Regression Performance:\n",
            "Accuracy: 0.8667\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.67      0.80         3\n",
            "           1       1.00      0.75      0.86         4\n",
            "           2       0.80      1.00      0.89         8\n",
            "\n",
            "    accuracy                           0.87        15\n",
            "   macro avg       0.93      0.81      0.85        15\n",
            "weighted avg       0.89      0.87      0.86        15\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2 0 1]\n",
            " [0 3 1]\n",
            " [0 0 8]]\n",
            "\n",
            "============================================================\n",
            "Training Support Vector Machine (Linear)...\n",
            "============================================================\n",
            "\n",
            "Support Vector Machine (Linear) Performance:\n",
            "Accuracy: 0.8667\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.67      0.67      0.67         3\n",
            "           1       0.80      1.00      0.89         4\n",
            "           2       1.00      0.88      0.93         8\n",
            "\n",
            "    accuracy                           0.87        15\n",
            "   macro avg       0.82      0.85      0.83        15\n",
            "weighted avg       0.88      0.87      0.87        15\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2 1 0]\n",
            " [0 4 0]\n",
            " [1 0 7]]\n",
            "\n",
            "============================================================\n",
            "Training Support Vector Machine (RBF)...\n",
            "============================================================\n",
            "\n",
            "Support Vector Machine (RBF) Performance:\n",
            "Accuracy: 0.8667\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.67      0.80         3\n",
            "           1       1.00      0.75      0.86         4\n",
            "           2       0.80      1.00      0.89         8\n",
            "\n",
            "    accuracy                           0.87        15\n",
            "   macro avg       0.93      0.81      0.85        15\n",
            "weighted avg       0.89      0.87      0.86        15\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2 0 1]\n",
            " [0 3 1]\n",
            " [0 0 8]]\n",
            "\n",
            "============================================================\n",
            "Training Random Forest...\n",
            "============================================================\n",
            "\n",
            "Random Forest Performance:\n",
            "Accuracy: 0.9333\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      0.67      0.80         3\n",
            "           1       0.80      1.00      0.89         4\n",
            "           2       1.00      1.00      1.00         8\n",
            "\n",
            "    accuracy                           0.93        15\n",
            "   macro avg       0.93      0.89      0.90        15\n",
            "weighted avg       0.95      0.93      0.93        15\n",
            "\n",
            "\n",
            "Confusion Matrix:\n",
            "[[2 1 0]\n",
            " [0 4 0]\n",
            " [0 0 8]]\n"
          ]
        }
      ],
      "source": [
        "# Define models to train\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(random_state=42, max_iter=1000),\n",
        "    \"Support Vector Machine (Linear)\": SVC(kernel='linear', random_state=42, probability=False),\n",
        "    \"Support Vector Machine (RBF)\": SVC(kernel='rbf', random_state=42, probability=False),\n",
        "    \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42, max_depth=10)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "\n",
        "for name, model in models.items():\n",
        "    acc = train_and_evaluate(model, name, X_train, Y_train, X_test, Y_test)\n",
        "    results[name] = acc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzn5moPsRywE"
      },
      "source": [
        "### Step 8: Comparing Models to Identify the Best One"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cDsM9P33RywE",
        "outputId": "e9bcc37d-8c26-4856-cfb6-28421ca18adb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "Summary of Model Accuracies:\n",
            "============================================================\n",
            "Random Forest: 0.9333\n",
            "Logistic Regression: 0.8667\n",
            "Support Vector Machine (Linear): 0.8667\n",
            "Support Vector Machine (RBF): 0.8667\n",
            "\n",
            "============================================================\n",
            "Best Performing Model: Random Forest\n",
            "============================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Summary of Model Accuracies:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "sorted_results = sorted(results.items(), key=lambda x: x[1], reverse=True)\n",
        "for name, acc in sorted_results:\n",
        "    print(f\"{name}: {acc:.4f}\")\n",
        "\n",
        "best_model_name = sorted_results[0][0]\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Best Performing Model: {best_model_name}\")\n",
        "print(f\"{'='*60}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xpHn4oRbRywE"
      },
      "source": [
        "### Step 9: Key Advantages of Transformer-Based Classification\n",
        "\n",
        "**Why use Sentence Transformers instead of TF-IDF?**\n",
        "\n",
        "1. **Semantic Understanding**: Transformers capture meaning and context, not just word frequencies\n",
        "2. **Dense Embeddings**: 384-dimensional vectors vs sparse TF-IDF matrices (more efficient)\n",
        "3. **Pre-trained Knowledge**: Leverages knowledge learned from millions of text examples\n",
        "4. **Better Generalization**: Often performs better on unseen data\n",
        "5. **Context-Aware**: Understands word relationships and sentence structure\n",
        "\n",
        "**Comparison with Notebook 5 (TF-IDF approach):**\n",
        "- Notebook 5: Uses sparse TF-IDF features → classical ML models\n",
        "- This notebook: Uses dense transformer embeddings → classical ML models\n",
        "- Both approaches can use the same classifiers, but embeddings provide richer semantic features"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}