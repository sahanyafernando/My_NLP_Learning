{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sahanyafernando/My_NLP_Learning/blob/main/Project_01_Public_Responce_Analysis/notebooks/Project_01_Public_Response_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load preprocessing artifacts\nLoads outputs saved by `01_data_loading_and_preprocessing.ipynb`. Run that notebook first if this file is missing."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "import pickle, pathlib\n",
        "# Update this path if your project lives elsewhere in Drive\n",
        "artifacts_root = pathlib.Path(\"/content/drive/MyDrive/My_NLP_Learning/Project_01_Public_Responce_Analysis\")\n",
        "artifacts_path = artifacts_root / \"artifacts/preprocessing_outputs.pkl\"\n",
        "if artifacts_path.exists():\n",
        "    with open(artifacts_path, \"rb\") as f:\n",
        "        artifacts = pickle.load(f)\n",
        "    df = artifacts[\"df\"]\n",
        "    one_hot_vectorizer = artifacts[\"one_hot_vectorizer\"]\n",
        "    one_hot_matrix = artifacts[\"one_hot_matrix\"]\n",
        "    bow_vectorizer = artifacts[\"bow_vectorizer\"]\n",
        "    bow_matrix = artifacts[\"bow_matrix\"]\n",
        "    tfidf_vectorizer = artifacts[\"tfidf_vectorizer\"]\n",
        "    tfidf_matrix = artifacts[\"tfidf_matrix\"]\n",
        "    cooccurrence_vectorizer = artifacts[\"cooccurrence_vectorizer\"]\n",
        "    cooccurrence_matrix = artifacts[\"cooccurrence_matrix\"]\n",
        "    print(\"Loaded preprocessing outputs from artifacts/preprocessing_outputs.pkl\")\n",
        "else:\n",
        "    print(\"Run 01_data_loading_and_preprocessing.ipynb to generate artifacts first.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dense Embeddings Creation\n",
        "Generate Word2Vec embeddings from the preprocessed text data (`cleaned_tokens`) using the `gensim` library. This will involve training a Word2Vec model and then displaying a sample of learned embeddings and demonstrating how to find similar words for a given term."
      ],
      "metadata": {
        "id": "VQ8ErfIvf6Ub"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim\n",
        "print(\"Gensim library installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CUphPMfbK2U-",
        "outputId": "34b492ad-8076-4426-ea8f-2a5f6706c975"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u2501\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Gensim library installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Initialize a Word2Vec model\n",
        "# vector_size: Dimension of the word embeddings\n",
        "# window: Maximum distance between the current and predicted word within a sentence\n",
        "# min_count: Ignores all words with total frequency lower than this\n",
        "# workers: Use these many worker threads to train the model (=faster training with multicore CPUs)\n",
        "model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)\n",
        "\n",
        "# Train the Word2Vec model on the 'cleaned_tokens' column\n",
        "# The 'cleaned_tokens' column already contains lists of words (sentences).\n",
        "model.build_vocab(df['cleaned_tokens'])\n",
        "model.train(df['cleaned_tokens'], total_examples=model.corpus_count, epochs=10)\n",
        "\n",
        "print(\"Word2Vec model trained successfully.\")\n",
        "\n",
        "# Display the vector for a sample word\n",
        "# Check if the vocabulary is not empty before attempting to access words\n",
        "if len(model.wv.key_to_index) > 0:\n",
        "    # Choose a word that is likely to be in the vocabulary, e.g., a top word from previous analysis\n",
        "    sample_word = \"educationpolicy\"\n",
        "    if sample_word in model.wv.key_to_index:\n",
        "        print(f\"\\nVector for '{sample_word}':\")\n",
        "        print(model.wv[sample_word])\n",
        "    else:\n",
        "        # If the chosen word is not in vocab, pick the first available word\n",
        "        sample_word = list(model.wv.key_to_index.keys())[0]\n",
        "        print(f\"\\n'{sample_word}' was not in vocabulary. Displaying vector for '{sample_word}':\")\n",
        "        print(model.wv[sample_word])\n",
        "\n",
        "    # Find and print the top 5 most similar words for a chosen term\n",
        "    if sample_word in model.wv.key_to_index:\n",
        "        print(f\"\\nTop 5 similar words to '{sample_word}':\")\n",
        "        try:\n",
        "            similar_words = model.wv.most_similar(sample_word, topn=5)\n",
        "            for word, similarity in similar_words:\n",
        "                print(f\"{word}: {similarity:.4f}\")\n",
        "        except KeyError:\n",
        "            print(f\"Could not find similar words for '{sample_word}' (possibly too infrequent).\")\n",
        "else:\n",
        "    print(\"\\nWord2Vec vocabulary is empty. Cannot display vectors or similar words.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWBx8NsNLBUE",
        "outputId": "3825757f-6365-46a4-d611-f98075095d0f"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec model trained successfully.\n",
            "\n",
            "Vector for 'educationpolicy':\n",
            "[-0.00888482  0.00406113  0.00528503  0.00613152  0.00766228 -0.00712012\n",
            "  0.00097215  0.00734187 -0.00319307 -0.00609324 -0.0010288  -0.00907577\n",
            " -0.00539532  0.0072668   0.00321684  0.00669209  0.00704312  0.00698161\n",
            " -0.00381457 -0.00140761  0.00271286 -0.00427031  0.00859728 -0.01029839\n",
            "  0.0066366   0.00299851 -0.0056163   0.00349566 -0.00205988  0.00716666\n",
            "  0.01083251 -0.00407961 -0.0006324  -0.00577595  0.00368347  0.00356117\n",
            "  0.0064988   0.00535338  0.00910546  0.00779381  0.00769153 -0.00743954\n",
            " -0.00939768 -0.00062215 -0.00242623  0.0073053   0.00563285 -0.00148196\n",
            "  0.00184443  0.00194863  0.00817344 -0.0102214  -0.00027507  0.0036235\n",
            " -0.00144402  0.00872862  0.00922212  0.00664775 -0.00145341  0.00798887\n",
            " -0.00845824  0.00342318 -0.00514189 -0.00560878  0.00283036  0.00561479\n",
            "  0.00805331 -0.00530009  0.00672292  0.00718346 -0.00436599 -0.00887923\n",
            "  0.00614025  0.00608644 -0.00036038 -0.00644362 -0.00701121 -0.00324434\n",
            "  0.00468812 -0.00329157 -0.00967109  0.00329807  0.00421188 -0.00527873\n",
            "  0.00129926 -0.00196505 -0.00013412 -0.00904726  0.00363363 -0.00502492\n",
            "  0.00166477 -0.00143032  0.0021551  -0.00755682 -0.00189666  0.00343626\n",
            "  0.00569741 -0.00321824 -0.00937957  0.00448368]\n",
            "\n",
            "Top 5 similar words to 'educationpolicy':\n",
            "religious: 0.3722\n",
            "form: 0.2343\n",
            "good: 0.2225\n",
            "society: 0.2223\n",
            "significant: 0.2205\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate FastText Embeddings\n",
        "Generate FastText embeddings from the preprocessed text data (`cleaned_tokens`) using the `gensim` library. This will involve training a FastText model and then displaying a sample of learned embeddings and demonstrating how to find similar words for a given term."
      ],
      "metadata": {
        "id": "AIiaNNtjL_Ii"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import FastText\n",
        "\n",
        "# Initialize a FastText model\n",
        "# vector_size: Dimension of the word embeddings\n",
        "# window: Maximum distance between the current and predicted word within a sentence\n",
        "# min_count: Ignores all words with total frequency lower than this\n",
        "# workers: Use these many worker threads to train the model (=faster training with multicore CPUs)\n",
        "fasttext_model = FastText(vector_size=100, window=5, min_count=1, workers=4, sg=1) # sg=1 for skip-gram, common for FastText\n",
        "\n",
        "# Train the FastText model on the 'cleaned_tokens' column\n",
        "# The 'cleaned_tokens' column already contains lists of words (sentences).\n",
        "fasttext_model.build_vocab(df['cleaned_tokens'])\n",
        "fasttext_model.train(df['cleaned_tokens'], total_examples=fasttext_model.corpus_count, epochs=10)\n",
        "\n",
        "print(\"FastText model trained successfully.\")\n",
        "\n",
        "# Display the vector for a sample word\n",
        "if len(fasttext_model.wv.key_to_index) > 0:\n",
        "    sample_word_ft = \"educationpolicy\"\n",
        "    if sample_word_ft in fasttext_model.wv.key_to_index:\n",
        "        print(f\"\\nVector for '{sample_word_ft}' (FastText):\")\n",
        "        print(fasttext_model.wv[sample_word_ft])\n",
        "    else:\n",
        "        sample_word_ft = list(fasttext_model.wv.key_to_index.keys())[0]\n",
        "        print(f\"\\n'{sample_word_ft}' was not in vocabulary. Displaying vector for '{sample_word_ft}' (FastText):\")\n",
        "        print(fasttext_model.wv[sample_word_ft])\n",
        "\n",
        "    # Find and print the top 5 most similar words for a chosen term\n",
        "    if sample_word_ft in fasttext_model.wv.key_to_index:\n",
        "        print(f\"\\nTop 5 similar words to '{sample_word_ft}' (FastText):\")\n",
        "        try:\n",
        "            similar_words_ft = fasttext_model.wv.most_similar(sample_word_ft, topn=5)\n",
        "            for word, similarity in similar_words_ft:\n",
        "                print(f\"{word}: {similarity:.4f}\")\n",
        "        except KeyError:\n",
        "            print(f\"Could not find similar words for '{sample_word_ft}' (possibly too infrequent).\")\n",
        "else:\n",
        "    print(\"\\nFastText vocabulary is empty. Cannot display vectors or similar words.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZmyTIREqLbgR",
        "outputId": "c1bf00ef-65e5-4da1-af91-7b9891df75d6"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FastText model trained successfully.\n",
            "\n",
            "Vector for 'educationpolicy' (FastText):\n",
            "[-0.02180171 -0.00214988 -0.00262839 -0.01025171  0.00643344  0.03016493\n",
            " -0.0005664  -0.00301163 -0.00150538 -0.02377232  0.00433622 -0.00604296\n",
            " -0.00388171  0.02126461 -0.0078538  -0.04925325 -0.024071    0.02225541\n",
            " -0.02101325 -0.01215765 -0.03873902  0.03926503 -0.04683848 -0.01634804\n",
            "  0.00625108 -0.00161708 -0.02398336  0.01321854  0.03458745  0.00321404\n",
            " -0.01849218  0.00056485  0.03159113 -0.01305179 -0.02635822  0.0062166\n",
            " -0.00345002  0.00716515 -0.03167881  0.00127246  0.02498819 -0.03717434\n",
            "  0.01392756 -0.02894269 -0.05074468 -0.03692038 -0.00187663 -0.02918363\n",
            " -0.02457221  0.00573102  0.00615972 -0.01595497  0.01608039 -0.00454777\n",
            " -0.01033915  0.0066973  -0.04249965 -0.01159947 -0.00989624 -0.0045944\n",
            "  0.01352564 -0.04472865 -0.00841846  0.01732462  0.02793944  0.03424306\n",
            "  0.00578606  0.02987581  0.00644182  0.01611372  0.00897061 -0.00127\n",
            "  0.00906626 -0.01583804  0.01422153  0.00200769  0.04073504  0.00849271\n",
            " -0.01596817  0.0232483  -0.01614689 -0.02553686 -0.01438639  0.01142007\n",
            " -0.00587056 -0.01621772 -0.00504923  0.00695372  0.00655474 -0.0079369\n",
            " -0.029224   -0.007025   -0.02132058  0.02666418 -0.0289836   0.03671834\n",
            " -0.00614453 -0.01670108  0.01373083  0.03556218]\n",
            "\n",
            "Top 5 similar words to 'educationpolicy' (FastText):\n",
            "education: 0.9995\n",
            "station: 0.9993\n",
            "generation: 0.9991\n",
            "operation: 0.9991\n",
            "national: 0.9991\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Apply Non-Negative Matrix Factorization (NMF) for Topic Modeling\n",
        "Apply Non-Negative Matrix Factorization (NMF) to the TF-IDF matrix to extract latent topics from the text data.\n"
      ],
      "metadata": {
        "id": "HNLyy1ZAM5gp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.decomposition import NMF\n",
        "\n",
        "# 2. Determine an appropriate number of topics\n",
        "n_topics = 5 # Example: choosing 5 topics\n",
        "\n",
        "# 3. Initialize an NMF model\n",
        "nmf_model = NMF(n_components=n_topics, random_state=42)\n",
        "\n",
        "# 4. Fit the NMF model to the tfidf_matrix and transform it\n",
        "doc_topic_matrix = nmf_model.fit_transform(tfidf_matrix)\n",
        "topic_word_matrix = nmf_model.components_\n",
        "\n",
        "# 5. Print the shape of both matrices\n",
        "print(f\"Shape of Document-Topic Matrix: {doc_topic_matrix.shape}\")\n",
        "print(f\"Shape of Topic-Word Matrix: {topic_word_matrix.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MJ5DyqGkNHKd",
        "outputId": "84b166df-23b7-4447-cfae-13a35f08f874"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of Document-Topic Matrix: (100, 5)\n",
            "Shape of Topic-Word Matrix: (5, 745)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Interpret NMF Topics\n",
        "\n",
        "Interpret the topics extracted by the NMF model by displaying the top words associated with each topic.\n"
      ],
      "metadata": {
        "id": "cB92tfnuNGuA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_names = tfidf_vectorizer.get_feature_names_out()\n",
        "n_top_words = 10\n",
        "\n",
        "print(\"\\nTopics in NMF model:\")\n",
        "for topic_idx, topic in enumerate(topic_word_matrix):\n",
        "    print(f\"Topic #{topic_idx + 1}:\")\n",
        "    # Sort words by their weights in descending order\n",
        "    top_words_indices = topic.argsort()[:-n_top_words - 1:-1]\n",
        "    top_words = [feature_names[i] for i in top_words_indices]\n",
        "    print(f\"{', '.join(top_words)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NA5v8StMNSu1",
        "outputId": "9569f349-a722-4d5c-cade-4455a3e5f911"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Topics in NMF model:\n",
            "Topic #1:\n",
            "economicrelief, paper, way, boy, face, difficult, market, officer, such, remember\n",
            "Topic #2:\n",
            "publictransport, although, worker, others, choice, job, reveal, prove, everyone, never\n",
            "Topic #3:\n",
            "healthcarereform, wait, state, doctor, add, seek, mr, move, agent, maintain\n",
            "Topic #4:\n",
            "educationpolicy, second, most, develop, plan, itself, mrs, car, much, reflect\n",
            "Topic #5:\n",
            "environmentallaws, current, occur, painting, various, hear, specific, generation, class, industry\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate GloVe Embeddings\n",
        "\n",
        "Generate GloVe embeddings from the preprocessed text data (`cleaned_tokens`) using an appropriate library. This will involve obtaining pre-trained GloVe vectors or training a GloVe model if necessary, then displaying a sample of learned embeddings and demonstrating how to find similar words for a given term.\n"
      ],
      "metadata": {
        "id": "cBRkfQmZNhwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import urllib.request\n",
        "import zipfile\n",
        "\n",
        "# Define paths and filenames\n",
        "glove_zip_file = 'glove.6B.zip'\n",
        "glove_txt_file = 'glove.6B.100d.txt'\n",
        "glove_url = 'http://nlp.stanford.edu/data/glove.6B.zip'\n",
        "\n",
        "# download the pre-trained GloVe embeddings (glove.6B.100d.txt)\n",
        "# Check if GloVe file exists, if not, download and extract\n",
        "if not os.path.exists(glove_txt_file):\n",
        "    if not os.path.exists(glove_zip_file):\n",
        "        print(f\"Downloading {glove_zip_file}...\")\n",
        "        urllib.request.urlretrieve(glove_url, glove_zip_file)\n",
        "        print(\"Download complete.\")\n",
        "\n",
        "    print(f\"Extracting {glove_txt_file} from {glove_zip_file}...\")\n",
        "    with zipfile.ZipFile(glove_zip_file, 'r') as zf:\n",
        "        zf.extract(glove_txt_file, path='.')\n",
        "    print(\"Extraction complete.\")\n",
        "else:\n",
        "    print(f\"{glove_txt_file} already exists. Skipping download and extraction.\")\n",
        "\n",
        "# Load the GloVe embeddings into a dictionary\n",
        "glove_embeddings = {}\n",
        "print(f\"Loading GloVe embeddings from {glove_txt_file}...\")\n",
        "with open(glove_txt_file, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        parts = line.split()\n",
        "        word = parts[0]\n",
        "        vector = np.array(parts[1:], dtype=np.float32)\n",
        "        glove_embeddings[word] = vector\n",
        "print(f\"Loaded {len(glove_embeddings)} GloVe embeddings.\")\n",
        "print(f\"Embedding dimension: {len(next(iter(glove_embeddings.values())))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRop8h8gNjiv",
        "outputId": "d5e15a35-f014-4f9d-ff80-d5719b2198e8"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading glove.6B.zip...\n",
            "Download complete.\n",
            "Extracting glove.6B.100d.txt from glove.6B.zip...\n",
            "Extraction complete.\n",
            "Loading GloVe embeddings from glove.6B.100d.txt...\n",
            "Loaded 400000 GloVe embeddings.\n",
            "Embedding dimension: 100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "define a function to retrieve word vectors, handle out-of-vocabulary words, and then use these embeddings to display the vector for a sample word ('educationpolicy') and find its top 5 most similar words using cosine similarity, as per the subtask instructions."
      ],
      "metadata": {
        "id": "vx0bsOWvPT0N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Function to get GloVe vector for a word\n",
        "def get_glove_vector(word, embeddings_dict, vector_dim=100):\n",
        "    return embeddings_dict.get(word, np.zeros(vector_dim)) # Return zero vector if word not found\n",
        "\n",
        "# Function to find most similar words\n",
        "def find_similar_words_glove(word, embeddings_dict, topn=5, vector_dim=100):\n",
        "    if word not in embeddings_dict:\n",
        "        return [] # Return empty list if the word itself is not in the vocabulary\n",
        "\n",
        "    word_vector = embeddings_dict[word].reshape(1, -1)\n",
        "\n",
        "    similarities = []\n",
        "    for vocab_word, vocab_vector in embeddings_dict.items():\n",
        "        if vocab_word == word: # Skip self-comparison\n",
        "            continue\n",
        "\n",
        "        # Reshape vocab_vector for cosine_similarity function\n",
        "        vocab_vector_reshaped = vocab_vector.reshape(1, -1)\n",
        "\n",
        "        similarity = cosine_similarity(word_vector, vocab_vector_reshaped)[0][0]\n",
        "        similarities.append((vocab_word, similarity))\n",
        "\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:topn]\n",
        "\n",
        "# Display the vector for a sample word\n",
        "sample_word_glove = \"educationpolicy\"\n",
        "\n",
        "# Check if the word exists in the GloVe vocabulary\n",
        "if sample_word_glove in glove_embeddings:\n",
        "    print(f\"\\nVector for '{sample_word_glove}' (GloVe):\")\n",
        "    print(glove_embeddings[sample_word_glove])\n",
        "else:\n",
        "    # If the chosen word is not in vocab, pick the first available word from our cleaned_tokens\n",
        "    # and ensure it exists in GloVe embeddings for demonstration\n",
        "    found_in_glove = False\n",
        "    for token_list in df['cleaned_tokens']:\n",
        "        for token in token_list:\n",
        "            if token in glove_embeddings:\n",
        "                sample_word_glove = token\n",
        "                found_in_glove = True\n",
        "                break\n",
        "        if found_in_glove:\n",
        "            break\n",
        "\n",
        "    if found_in_glove:\n",
        "        print(f\"\\n'{sample_word_glove}' from our data was found in GloVe. Displaying vector:\")\n",
        "        print(glove_embeddings[sample_word_glove])\n",
        "    else:\n",
        "        print(\"\\nCould not find any common words between cleaned_tokens and GloVe vocabulary.\")\n",
        "\n",
        "# Find and print the top 5 most similar words for a chosen term\n",
        "if sample_word_glove in glove_embeddings:\n",
        "    print(f\"\\nTop 5 similar words to '{sample_word_glove}' (GloVe):\")\n",
        "    similar_words_glove = find_similar_words_glove(sample_word_glove, glove_embeddings, topn=5)\n",
        "    if similar_words_glove:\n",
        "        for word, similarity in similar_words_glove:\n",
        "            print(f\"{word}: {similarity:.4f}\")\n",
        "    else:\n",
        "        print(f\"Could not find similar words for '{sample_word_glove}' (not in GloVe vocabulary).\")\n",
        "else:\n",
        "    print(\"Cannot find similar words as the sample word is not in GloVe vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "amAzWEQDPJYD",
        "outputId": "01a9992a-7b9a-4bee-d321-53dd79d6d3a7"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "'agent' from our data was found in GloVe. Displaying vector:\n",
            "[ 5.4263e-01 -9.4556e-01  6.3338e-01 -1.1186e+00 -3.3968e-02 -5.0336e-01\n",
            "  2.6620e-01 -4.3339e-01 -6.8826e-02  1.3260e-01 -1.1673e-01 -7.4280e-01\n",
            "  2.4098e-01  3.1927e-01  1.4900e-01  2.5225e-01  8.9375e-01  1.3620e-01\n",
            " -2.0531e-01  2.2922e-01 -4.5712e-01  3.7074e-02 -9.4237e-01 -2.0870e-01\n",
            "  1.0498e-01 -2.6186e-02  3.9272e-01 -6.1634e-01 -3.3848e-02  5.2247e-01\n",
            " -9.9738e-02  1.4282e+00 -9.5186e-02 -5.8463e-01  1.9847e-01 -2.1672e-01\n",
            " -6.6678e-01  2.5273e-01  6.6437e-01  1.1844e-01  4.9575e-01 -2.0699e-01\n",
            "  5.4199e-01 -2.2103e-01  8.4711e-01 -1.7925e-01 -8.5337e-01 -8.4244e-01\n",
            "  2.2508e-01 -2.3347e-01  1.0846e+00  3.4772e-01  1.7441e-01  7.9287e-01\n",
            "  1.6292e-01 -1.2031e+00 -8.5298e-01 -2.0582e-01  1.0274e+00  8.1467e-01\n",
            "  2.3544e-02  9.9344e-01 -2.2389e-01 -5.0794e-01  6.1108e-01 -8.4063e-01\n",
            "  1.1529e+00  7.7760e-02  1.0547e-01  5.7986e-01 -4.7526e-01 -1.0866e+00\n",
            " -7.3660e-01 -3.4891e-01  1.2625e-01 -3.9275e-01  1.0230e-01 -3.3372e-01\n",
            " -1.0588e+00 -5.8325e-02  8.8322e-01  7.7251e-01 -3.7973e-01 -2.6777e-01\n",
            " -7.1694e-01 -1.8102e-03  8.2903e-01  4.7776e-01 -5.6319e-02 -6.0168e-01\n",
            " -6.5215e-01 -1.7447e-01 -1.5826e-01 -2.0610e-01  3.8677e-01  3.1808e-01\n",
            " -3.2460e-04  3.4315e-01 -4.9299e-01 -4.4456e-01]\n",
            "\n",
            "Top 5 similar words to 'agent' (GloVe):\n",
            "agents: 0.7551\n",
            "fbi: 0.6515\n",
            "undercover: 0.6378\n",
            "informant: 0.6036\n",
            "manager: 0.5806\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Byte-Pair Encoding (BPE) Embeddings\n",
        "Generate Byte-Pair Encoding (BPE) subword embeddings from the preprocessed text data."
      ],
      "metadata": {
        "id": "zr2InQEdPmej"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tokenizers\n",
        "print(\"Tokenizers library installed.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lf85pessP3DG",
        "outputId": "aa884226-473c-4720-9bbf-31e0df066932"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.12/dist-packages (0.22.1)\n",
            "Requirement already satisfied: huggingface-hub<2.0,>=0.16.4 in /usr/local/lib/python3.12/dist-packages (from tokenizers) (0.36.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (6.0.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<2.0,>=0.16.4->tokenizers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub<2.0,>=0.16.4->tokenizers) (2025.11.12)\n",
            "Tokenizers library installed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tokenizers import BertWordPieceTokenizer\n",
        "from gensim.models import Word2Vec\n",
        "import collections\n",
        "\n",
        "# Prepare the text data for BPE training\n",
        "# The tokenizer expects an iterator of sentences (strings)\n",
        "text_for_bpe_training = df['preprocessed_text_string'].tolist()\n",
        "\n",
        "# Initialize and train a BPE tokenizer\n",
        "# BertWordPieceTokenizer is a good choice for BPE-like subword tokenization\n",
        "tokenizer = BertWordPieceTokenizer(\n",
        "    clean_text=True,\n",
        "    handle_chinese_chars=False, # Assuming no Chinese characters based on initial data exploration\n",
        "    strip_accents=True,\n",
        "    lowercase=True,\n",
        ")\n",
        "\n",
        "# Train the tokenizer\n",
        "tokenizer.train_from_iterator(\n",
        "    text_for_bpe_training,\n",
        "    vocab_size=30000, # Increased vocab size to potentially capture more subwords\n",
        "    min_frequency=2, # Minimum frequency for words to be included in vocab\n",
        "    show_progress=True,\n",
        "    special_tokens=[\"[UNK]\", \"[CLS]\", \"[SEP]\", \"[PAD]\", \"[MASK]\"]\n",
        ")\n",
        "\n",
        "# Use the trained BPE tokenizer to tokenize the preprocessed_text_string column\n",
        "def tokenize_with_bpe(text):\n",
        "    output = tokenizer.encode(text)\n",
        "    return output.tokens\n",
        "\n",
        "df['bpe_tokens'] = df['preprocessed_text_string'].apply(tokenize_with_bpe)\n",
        "\n",
        "print(\"BPE tokenizer trained and 'bpe_tokens' column added to DataFrame.\")\n",
        "print(\"Displaying head of DataFrame with new 'bpe_tokens' column:\")\n",
        "print(df[['preprocessed_text_string', 'bpe_tokens']].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuFtRNWdSf6o",
        "outputId": "755217a1-7c41-4d0e-ae9f-c26f7f16f534"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BPE tokenizer trained and 'bpe_tokens' column added to DataFrame.\n",
            "Displaying head of DataFrame with new 'bpe_tokens' column:\n",
            "                            preprocessed_text_string  \\\n",
            "0  agent every development say quality throughout...   \n",
            "1   serve civil institution everyone publictransport   \n",
            "2  benefit suggest page southern role movie win n...   \n",
            "3  law street class great prove reduce raise auth...   \n",
            "4  detail food shoulder argue start source husban...   \n",
            "\n",
            "                                          bpe_tokens  \n",
            "0  [agent, every, develop, ##ment, say, quality, ...  \n",
            "1  [serve, civil, institution, everyone, publictr...  \n",
            "2  [be, ##n, ##ef, ##it, suggest, page, southern,...  \n",
            "3  [law, st, ##ree, ##t, class, great, prove, red...  \n",
            "4  [detail, food, shoulder, ar, ##g, ##ue, start,...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.models import Word2Vec\n",
        "\n",
        "# 5. Train a gensim.Word2Vec model on the 'bpe_tokens' column\n",
        "# The 'bpe_tokens' column already contains lists of subwords (sentences).\n",
        "bpe_word2vec_model = Word2Vec(vector_size=100, window=5, min_count=1, workers=4)\n",
        "bpe_word2vec_model.build_vocab(df['bpe_tokens'])\n",
        "bpe_word2vec_model.train(df['bpe_tokens'], total_examples=bpe_word2vec_model.corpus_count, epochs=10)\n",
        "\n",
        "print(\"Word2Vec model trained on BPE tokens successfully.\")\n",
        "\n",
        "# 6. Display a sample BPE subword embedding\n",
        "# Check if the vocabulary is not empty before attempting to access words\n",
        "if len(bpe_word2vec_model.wv.key_to_index) > 0:\n",
        "    # Pick a common BPE subword. 'education' is a good candidate from previous analysis.\n",
        "    sample_bpe_subword = \"education\"\n",
        "    if sample_bpe_subword in bpe_word2vec_model.wv.key_to_index:\n",
        "        print(f\"\\nVector for BPE subword '{sample_bpe_subword}':\")\n",
        "        print(bpe_word2vec_model.wv[sample_bpe_subword])\n",
        "    else:\n",
        "        # Fallback to the first available subword if 'education' is not in vocab\n",
        "        sample_bpe_subword = list(bpe_word2vec_model.wv.key_to_index.keys())[0]\n",
        "        print(f\"\\n'{sample_bpe_subword}' was not in BPE vocabulary. Displaying vector for '{sample_bpe_subword}':\")\n",
        "        print(bpe_word2vec_model.wv[sample_bpe_subword])\n",
        "\n",
        "    # 7. Find and print the top 5 most similar BPE subwords for the chosen sample subword\n",
        "    if sample_bpe_subword in bpe_word2vec_model.wv.key_to_index:\n",
        "        print(f\"\\nTop 5 similar BPE subwords to '{sample_bpe_subword}':\")\n",
        "        try:\n",
        "            similar_bpe_subwords = bpe_word2vec_model.wv.most_similar(sample_bpe_subword, topn=5)\n",
        "            for word, similarity in similar_bpe_subwords:\n",
        "                print(f\"{word}: {similarity:.4f}\")\n",
        "        except KeyError:\n",
        "            print(f\"Could not find similar BPE subwords for '{sample_bpe_subword}' (possibly too infrequent).\")\n",
        "else:\n",
        "    print(\"\\nBPE Word2Vec vocabulary is empty. Cannot display vectors or similar subwords.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ugAbVOqTUPu",
        "outputId": "28d10ead-107c-4993-bd16-59bd60ae84ca"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word2Vec model trained on BPE tokens successfully.\n",
            "\n",
            "Vector for BPE subword 'education':\n",
            "[ 4.1449177e-03 -1.0060999e-03 -2.8769341e-03 -6.9859810e-03\n",
            " -9.9111143e-03  7.1347994e-03  7.6038316e-03  1.0216403e-02\n",
            " -7.9620266e-03  7.4384669e-03  7.5606173e-03  4.1001765e-03\n",
            " -9.7969752e-03  1.3557842e-03  6.9591422e-03  5.7924306e-03\n",
            " -2.0944480e-04 -3.4422602e-03 -7.2046281e-03 -1.8854584e-03\n",
            "  1.0077420e-02  1.7108109e-03  8.1060181e-04  4.2135911e-03\n",
            "  7.1855192e-04 -5.4841670e-03 -1.6661159e-04 -1.0205462e-02\n",
            "  2.9596600e-03 -7.9015046e-03  9.3029784e-03  4.3631825e-03\n",
            "  1.8954043e-03 -2.5742857e-03  3.4966755e-03 -7.3880595e-03\n",
            "  4.8619844e-03 -9.2516998e-03 -1.5971689e-03 -1.0558230e-02\n",
            "  7.1480931e-03  1.4942370e-03  1.4381642e-04  4.3144426e-03\n",
            " -3.5410291e-03 -3.6841575e-03  6.7394222e-03  5.5679237e-03\n",
            "  9.1663823e-03 -3.1387031e-03  4.6052951e-03  1.0650485e-04\n",
            " -5.6375782e-03  8.3988084e-04  2.3408670e-03 -1.4302431e-04\n",
            "  7.0589781e-03  3.8767247e-03  5.9767631e-03  5.3352020e-03\n",
            " -7.2046919e-03 -9.8386044e-03  2.2502348e-03 -5.6249439e-03\n",
            "  6.9719921e-03  6.1529540e-03  8.6484840e-03 -1.1126795e-03\n",
            "  1.0314564e-03  1.0269401e-02  1.3577357e-03  1.1656190e-03\n",
            "  8.8512758e-03 -6.3911872e-04  4.7378195e-03 -2.3128409e-03\n",
            "  4.7490308e-03  5.3742677e-03  4.2127250e-03 -8.5511049e-03\n",
            " -8.5790120e-03  2.4478510e-03  8.2025835e-03  9.5933666e-03\n",
            "  9.0803159e-03 -9.9926940e-05 -4.5496495e-03  1.0415323e-02\n",
            "  1.9805219e-03 -9.0594972e-03  1.0611980e-03  4.8011583e-03\n",
            "  9.7854324e-03  3.0027488e-03 -1.4446328e-03 -7.8118909e-03\n",
            "  7.5276929e-04  7.8241415e-03 -8.3024474e-03 -2.0530808e-03]\n",
            "\n",
            "Top 5 similar BPE subwords to 'education':\n",
            "develop: 0.2974\n",
            "econom: 0.2949\n",
            "service: 0.2824\n",
            "fa: 0.2790\n",
            "doctor: 0.2589\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Unigram Language Models Embeddings\n",
        "\n",
        "Generate Unigram Language Models embeddings from the preprocessed text data (`cleaned_tokens`). This will involve calculating word probabilities based on unigram frequencies, displaying a sample word's probability, and identifying words with similar probabilities.\n"
      ],
      "metadata": {
        "id": "1K8fu_tPTJy4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import collections\n",
        "\n",
        "# 1. Flatten the 'cleaned_tokens' column into a single list of all words\n",
        "all_cleaned_words = [token for sublist in df['cleaned_tokens'] for token in sublist]\n",
        "\n",
        "# 2. Use collections.Counter to calculate the frequency of each unique word\n",
        "word_frequencies = collections.Counter(all_cleaned_words)\n",
        "\n",
        "print(\"Total unique words:\", len(word_frequencies))\n",
        "print(\"Total words in corpus:\", len(all_cleaned_words))\n",
        "print(\"Sample word frequencies (top 10):\")\n",
        "for word, freq in word_frequencies.most_common(10):\n",
        "    print(f\"{word}: {freq}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uxx6Y-UvTboS",
        "outputId": "ab257942-b2fe-4afa-a7cb-0461cdaf5de3"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total unique words: 746\n",
            "Total words in corpus: 1463\n",
            "Sample word frequencies (top 10):\n",
            "economicrelief: 24\n",
            "educationpolicy: 23\n",
            "healthcarereform: 21\n",
            "publictransport: 19\n",
            "environmentallaws: 13\n",
            "current: 6\n",
            "police: 6\n",
            "law: 5\n",
            "boy: 5\n",
            "rest: 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "compute the unigram probability for each word, select a sample word ('educationpolicy') to display its probability, and then identify and present the top 5 words with the most similar probabilities to illustrate unigram embeddings."
      ],
      "metadata": {
        "id": "iOYVPW5Zb255"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "total_words = len(all_cleaned_words)\n",
        "unigram_probabilities = {word: freq / total_words for word, freq in word_frequencies.items()}\n",
        "\n",
        "# 4. Choose a sample word and display its calculated unigram probability\n",
        "sample_word_unigram = \"educationpolicy\"\n",
        "if sample_word_unigram in unigram_probabilities:\n",
        "    print(f\"\\nUnigram probability for '{sample_word_unigram}': {unigram_probabilities[sample_word_unigram]:.6f}\")\n",
        "else:\n",
        "    print(f\"\\n'{sample_word_unigram}' not found in vocabulary.\")\n",
        "\n",
        "# 5. Identify and display the top 5 words that have the most similar unigram probabilities to the sample word\n",
        "if sample_word_unigram in unigram_probabilities:\n",
        "    sample_prob = unigram_probabilities[sample_word_unigram]\n",
        "\n",
        "    # Calculate absolute difference in probabilities\n",
        "    similarity_scores = []\n",
        "    for word, prob in unigram_probabilities.items():\n",
        "        if word == sample_word_unigram:\n",
        "            continue\n",
        "        similarity_scores.append((word, abs(prob - sample_prob)))\n",
        "\n",
        "    # Sort by the absolute difference (smallest difference means highest similarity)\n",
        "    similarity_scores.sort(key=lambda x: x[1])\n",
        "\n",
        "    print(f\"\\nTop 5 words with most similar unigram probabilities to '{sample_word_unigram}':\")\n",
        "    for i, (word, diff) in enumerate(similarity_scores[:5]):\n",
        "        print(f\"{word}: (Probability: {unigram_probabilities[word]:.6f}, Abs Diff: {diff:.6f})\")\n",
        "else:\n",
        "    print(f\"Cannot find similar words as '{sample_word_unigram}' is not in the unigram vocabulary.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DmA2snX2b4HP",
        "outputId": "53955c58-3191-4560-8e60-38c645a851ad"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Unigram probability for 'educationpolicy': 0.015721\n",
            "\n",
            "Top 5 words with most similar unigram probabilities to 'educationpolicy':\n",
            "economicrelief: (Probability: 0.016405, Abs Diff: 0.000684)\n",
            "healthcarereform: (Probability: 0.014354, Abs Diff: 0.001367)\n",
            "publictransport: (Probability: 0.012987, Abs Diff: 0.002734)\n",
            "environmentallaws: (Probability: 0.008886, Abs Diff: 0.006835)\n",
            "current: (Probability: 0.004101, Abs Diff: 0.011620)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Generate Hybrid Character+Word Embeddings\n",
        "\n",
        "Generate hybrid character+word embeddings from the preprocessed text data (`cleaned_tokens`) by combining character n-gram features with existing Word2Vec embeddings.\n"
      ],
      "metadata": {
        "id": "np0b5-twcAq-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# 1. Aggregate all unique words from the cleaned_tokens column\n",
        "# Flatten the list of lists into a single list of words and get unique words\n",
        "all_unique_words = list(set([word for sublist in df['cleaned_tokens'] for word in sublist]))\n",
        "print(f\"Number of unique words: {len(all_unique_words)}\")\n",
        "\n",
        "# 2. Initialize a CountVectorizer with analyzer='char' and ngram_range=(2,3)\n",
        "char_ngram_vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 3))\n",
        "\n",
        "# Fit this vectorizer on the unique words to create a character n-gram vocabulary\n",
        "char_ngram_vectorizer.fit(all_unique_words)\n",
        "print(\"Character n-gram vectorizer fitted.\")\n",
        "print(f\"Number of character n-gram features: {len(char_ngram_vectorizer.get_feature_names_out())}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yKT9J9EcDB8",
        "outputId": "f5e0bc08-afbc-4835-964b-6d3cba6c751f"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of unique words: 746\n",
            "Character n-gram vectorizer fitted.\n",
            "Number of character n-gram features: 1505\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "create a dictionary to store hybrid embeddings by iterating through each unique word, obtaining its character n-gram representation and its Word2Vec embedding, concatenating them, and storing the result.\n",
        "\n"
      ],
      "metadata": {
        "id": "t_joQLh3cPhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "word_vector_dim = model.vector_size # Word2Vec vector dimension (100)\n",
        "char_ngram_vector_dim = len(char_ngram_vectorizer.get_feature_names_out())\n",
        "\n",
        "# 3. Create a dictionary to store the hybrid embeddings\n",
        "hybrid_embeddings = {}\n",
        "\n",
        "for word in all_unique_words:\n",
        "    # a. Get its character n-gram representation\n",
        "    # Transform the word using the fitted CountVectorizer\n",
        "    char_ngram_representation = char_ngram_vectorizer.transform([word]).toarray()[0]\n",
        "\n",
        "    # b. Retrieve its corresponding Word2Vec embedding\n",
        "    word_embedding = model.wv[word] if word in model.wv else np.zeros(word_vector_dim)\n",
        "\n",
        "    # c. Concatenate the character n-gram vector and the Word2Vec embedding\n",
        "    hybrid_embedding = np.concatenate((char_ngram_representation, word_embedding))\n",
        "\n",
        "    # Store this in your dictionary\n",
        "    hybrid_embeddings[word] = hybrid_embedding\n",
        "\n",
        "print(f\"Generated hybrid embeddings for {len(hybrid_embeddings)} unique words.\")\n",
        "print(f\"Dimension of hybrid embeddings: {len(next(iter(hybrid_embeddings.values())))}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjMhN3NicHK3",
        "outputId": "6207898b-8e6a-41f3-81f8-7d745cfdf0db"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Generated hybrid embeddings for 746 unique words.\n",
            "Dimension of hybrid embeddings: 1605\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "display the vector for a sample word ('educationpolicy') and then implement a function to find and display the top 5 most similar words based on cosine similarity of their hybrid embeddings, as per the subtask instructions."
      ],
      "metadata": {
        "id": "VUOjWWEfcWXs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# 4. Display the hybrid embedding vector for a sample word\n",
        "sample_word_hybrid = \"educationpolicy\"\n",
        "if sample_word_hybrid in hybrid_embeddings:\n",
        "    print(f\"\\nHybrid embedding for '{sample_word_hybrid}':\")\n",
        "    print(hybrid_embeddings[sample_word_hybrid])\n",
        "else:\n",
        "    print(f\"\\n'{sample_word_hybrid}' not found in hybrid embeddings vocabulary.\")\n",
        "\n",
        "# 5. Implement a function to find the top 5 most similar words based on cosine similarity\n",
        "def find_similar_hybrid_words(word, hybrid_embeddings_dict, topn=5):\n",
        "    if word not in hybrid_embeddings_dict:\n",
        "        return []\n",
        "\n",
        "    word_vector = hybrid_embeddings_dict[word].reshape(1, -1)\n",
        "\n",
        "    # Calculate cosine similarity with all other words\n",
        "    similarities = []\n",
        "    for vocab_word, vocab_vector in hybrid_embeddings_dict.items():\n",
        "        if vocab_word == word:\n",
        "            continue\n",
        "\n",
        "        vocab_vector_reshaped = vocab_vector.reshape(1, -1)\n",
        "        similarity = cosine_similarity(word_vector, vocab_vector_reshaped)[0][0]\n",
        "        similarities.append((vocab_word, similarity))\n",
        "\n",
        "    # Sort by similarity in descending order\n",
        "    similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "    return similarities[:topn]\n",
        "\n",
        "# Demonstrate finding similar words for the chosen sample word\n",
        "if sample_word_hybrid in hybrid_embeddings:\n",
        "    print(f\"\\nTop 5 similar words to '{sample_word_hybrid}' (Hybrid Embeddings):\")\n",
        "    similar_hybrid_words = find_similar_hybrid_words(sample_word_hybrid, hybrid_embeddings, topn=5)\n",
        "    if similar_hybrid_words:\n",
        "        for word, similarity in similar_hybrid_words:\n",
        "            print(f\"{word}: {similarity:.4f}\")\n",
        "    else:\n",
        "        print(f\"Could not find similar words for '{sample_word_hybrid}'.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cah4rAcoceyp",
        "outputId": "51949729-b07f-4cee-8f2b-1b8a43c17f8b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Hybrid embedding for 'educationpolicy':\n",
            "[ 0.          0.          0.         ... -0.00321824 -0.00937957\n",
            "  0.00448368]\n",
            "\n",
            "Top 5 similar words to 'educationpolicy' (Hybrid Embeddings):\n",
            "education: 0.7453\n",
            "policy: 0.5772\n",
            "police: 0.4489\n",
            "station: 0.4061\n",
            "political: 0.3975\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNlbEDktmhmgYLf9YCV02x/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}